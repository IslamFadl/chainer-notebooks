{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec: Obtain word embeddings\n",
    "\n",
    "## 0. Introduction\n",
    "\n",
    "**Word2vec** is the tool for generating the distributed representation of words, which is proposed by Mikolov et al[[1]](#1). When the tool assigns a real-valued vector to each word, the closer the meanings of the words, the greater similarity the vectors will indicate.\n",
    "\n",
    "**Distributed representation** means assigning a real-valued vector for each object and representing the object by the vector. When representing a word by distributed representation, we call it **word embeddings**. In this tutorial, we will use the term.\n",
    "\n",
    "Let's think about what the meaning of word is. Since we are human, so we can understand that the words \"animal\" and \"dog\" are related. But what information will Word2vec use in order to learn the vectors for meanings of words? The words \"animal\" and \"dog\" should have similar vectors, but the words \"food\" and \"dog\" should be far from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Idea\n",
    "\n",
    "Word2vec learns the similarity of word meanings from simple information. It learns from a sequence of words in sentences. The idea is that the meaning of the word is determined by the words around it. This idea is based on **distributional hypothesis**[[2]](#2). The word to be learned is called the **\"center word\"**, and the words around it are called **\"context words\"**. Depending on the window size `c`, the number of Context Words will change.\n",
    "\n",
    "Here, let's see the algorithm by using this example sentence: \"**The cute cat jumps over the lazy dog.**\"\n",
    "\n",
    "- All of the following figures consider \"cat\" as the center word.\n",
    "- According to the window size `c`, you can see that context words will change.\n",
    "\n",
    "![](center_context_word.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Main Algorithm\n",
    "\n",
    "Word2vec, the tool for creating the word embeddings, is actually built with two models, which are called **Skip-gram** and **CBoW**.\n",
    "\n",
    "To explain the models with the figures below, we will use the following symbols.\n",
    "\n",
    "| Symbol    | Definition                                               |\n",
    "| --------: | :------------------------------------------------------- |\n",
    "| $V$       | The size of vocabulary                                   |\n",
    "| $D$       | The size of embedding vector                             |\n",
    "| $v_t$     | A one-hot center word vector                             |\n",
    "| $v_{t \\pm c \\backslash 0}$ | $c$ context vectors around $v_t$        |\n",
    "| $l_H$     | An embedding vector of an input word vector              |\n",
    "| $l_O$     | An output vector of the network                          |\n",
    "| $W_H$     | The embedding matrix for inputs                          |\n",
    "| $W_O$     | The embedding matrix for outputs                         |\n",
    "\n",
    "**Note**\n",
    "\n",
    "It is common to use **negative sampling** or **hierarchical softmax** for the loss function, however, in this tutorial, we will use the **softmax over all words** and skip the other variants because of simplifying the explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Skip-gram\n",
    "\n",
    "This model learns to predict context words $v_{t+c}$ when a center word $v_t$ is given. In the model, each row of the embedding matrix for input $W_H$ becomes a word embedding of each word.\n",
    "\n",
    "When you input a center word $v_t$ into the network, you can calculate $\\hat{v}_{t+c}$ as follows:\n",
    "\n",
    "1. Calculate an embedding vector of the input center word vector: $l_H = W_H v_t$\n",
    "2. Calculate an output vector of the embedding vector: $l_O = W_O l_H$\n",
    "3. Calculate a probability vector of context words: $\\hat{v}_{t+c} = \\text{softmax}(l_O)$\n",
    "\n",
    "Each element of the $V$-dimensional vector $\\hat{v}_{t+c}$ is a probability that $i$-th word $w_i$ in the vocabulary is one of context words inside the $c$-sized window centered at the word $v_t$. So, the probability $p(v_{t+c} \\mid v_t)$ can be calculated by a dot product of a one-hot word vector $v_{t+c}$ and the output vector $\\hat{v}_{t+c}$.\n",
    "\n",
    "$p(v_{t+c} \\mid v_t) = v_{t+c}^T \\hat{v}_{t+c}$\n",
    "\n",
    "The loss function for the center word and the context words $\\text{loss}(v_{t-c}, ..., v_t, ..., v_{t+c}; W_H, W_O)$ is,\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "&&\\text{loss}(v_{t-c}, ..., v_t, ..., v_{t+c}; W_H, W_O) \\\\\n",
    "&=& \\sum_{i=\\{-c,...,c\\}/\\{0\\}} -\\log(p(v_{t+i}|v_t)) \\\\\n",
    "&=& \\sum_{i=\\{-c,...,c\\}/\\{0\\}} -\\log(v_{t+i}^T \\hat{v}_{t+i})\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "Let the training dataset be $\\mathcal{D}=\\{v_{t-c}^{(n)}, ..., v_t^{(n)}, ..., v_{t+c}^{(n)}\\}_{n=1}^N$, the loss over the dataset $\\text{Loss}(W_H, W_O|\\mathcal{D})$ is,\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "&&\\text{Loss}(\\mathcal{D}; W_H, W_O) \\\\\n",
    "&=& \\sum_{\\mathcal{D}} \\text{loss}(v_{t-c}, ..., v_t, ..., v_{t+c}; W_H, W_O) \\\\\n",
    "&=& \\sum_{\\mathcal{D}} \\sum_{i=\\{-c,...,c\\}/\\{0\\}} -\\log(v_{t+i}^T \\hat{v}_{t+c})\n",
    "\\end{eqnarray}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Continuous Bag of Words (CBoW)\n",
    "\n",
    "This model learns to predict the center word $v_t$ when context words $v_{t+i}(i \\in \\{-c,\\dots,c\\}\\backslash 0)$ is given. In this model, each column of the embedding matrix $W_O$ represents a word embedding vector of each word.\n",
    "\n",
    "When you give a set of context words $v_{t+i}(i \\in \\{-c,\\dots,c\\}\\backslash 0)$ to the network, you can calculate the probability of the center word $\\hat{v}_t$ as follows:\n",
    "\n",
    "1. Calculate a mean embedding vector from context words: $l_H = \\frac{1}{2c} \\sum_{i \\in \\{-c,...,c\\}\\backslash 0} W_H v_{t+i}$\n",
    "2. Calculate an output vector: $l_O = W_O l_H$\n",
    "3. Calculate an probability vector: $\\hat{v}_t = \\text{softmax}(l_O)$\n",
    "\n",
    "Each element of $\\hat{v}_t$ is a probability that the $i$-th word $w_i$ in the vocabulary is considered as the center word. So, the center word prediction $p(v_t \\mid v_{t-c}, \\dots v_{t-1}, v_{t+1}, \\dots, v_{t+c})$ can be calculated by $v_t^T \\hat{v}_t$, where $v_t$ denots the one-hot vector of the center word label vector.\n",
    "\n",
    "The loss function for the center word prediction is,\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "&& \\text{loss}(v_t|v_{t \\pm c \\backslash 0}; W_H, W_O) \\\\\n",
    "&=& \\sum_{i \\in \\{-c,...,c\\}\\backslash 0} -\\log(p(v_t|v_{t \\pm C \\backslash 0})) \\\\\n",
    "&=& -\\log(v_t^T \\hat{v}_t)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "Let the training dataset be $\\mathcal{D}=\\{v_{t-C}^{(n)}, \\dots, v_t^{(n)}, \\dots, v_{t+C}^{(n)}\\}_{n=1}^N$, the loss functions for dataset $\\text{Loss}(W_H, W_O|\\mathcal{D})$ is,\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "\\text{Loss}(W_H, W_O|\\mathcal{D})\n",
    "&=& \\sum_{\\mathcal{D}} \\text{loss}(W_H, W_O|v_{t-C}, ..., v_t, ..., v_{t+C}) \\\\\n",
    "&=& \\sum_{\\mathcal{D}} \\log(v_t^T v_t^*)\n",
    "\\end{eqnarray}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Details of Skip-gram\n",
    "\n",
    "In this tutorial, we mainly explain Skip-gram from the following viewpoints.\n",
    "\n",
    "1. It is easier to understand the algorithm than CBoW.\n",
    "2. Even if the number of words increases, the accuracy is largely maintained. So, it is more scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Example\n",
    "\n",
    "In this example, we use the following setups.\n",
    "\n",
    "* The size of vocabulary $N$ is 10.\n",
    "* The size of embedding vector $D$ is 2.\n",
    "* Center word is \"dog\".\n",
    "* Context word is \"animal\".\n",
    "\n",
    "Since there should be more than one Context Word, repeat the following process for each Context Word.\n",
    "\n",
    "1. The one-hot vector of \"dog\" is `[0 0 1 0 0 0 0 0 0 0]` and you input it as Center Word.\n",
    "2. After that, the third row of embedding matrix $W_H$ for Center Word is the word embedding of \"dog\" $L_H$.\n",
    "3. The output layer $L_O$ is the result of multiplying the embedding matrix $W_O$ for Context Words by the embedding vector of \"dog\" $L_H$.\n",
    "4. In order to limit the value of each element of the output layer,  softmax function is applied to the output layer $L_O$ to calculate $\\text{softmax}(L_O)$. Softmax function normalizes scores in the output layer $L_O$ into sum 1 to see the scores as probability distribution over all words.\n",
    "5. Calculate the error between $W_O$ and \"animal\"'s one-hot vector `[1 0 0 0 0 0 0 0 0 0 0]`, and propagate the error back to the network to update the parameters.\n",
    "\n",
    "![](skipgram_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation of Skip-gram by Chainer\n",
    "\n",
    "There is an example related to Word2vec on the GitHub repository, so we will explain based on that.\n",
    "\n",
    "- [chainer/examples/word2vec](https://github.com/chainer/chainer/tree/master/examples/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Implementation Method\n",
    "\n",
    "First, let's import necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import six\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda\n",
    "import chainer.functions as F\n",
    "import chainer.initializers as I\n",
    "import chainer.links as L\n",
    "import chainer.optimizers as O\n",
    "from chainer import reporter\n",
    "from chainer import training\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Basically, if you use chainer, you import in this way.\n",
    "* Importing functions as ``F`` and links as ``L`` makes it easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Network Structures\n",
    "\n",
    "Next, we define the network structures of skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_vocab, n_units, loss_func):\n",
    "        super(SkipGram, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.embed = L.EmbedID(\n",
    "                n_vocab, n_units, initialW=I.Uniform(1. / n_units))\n",
    "            self.loss_func = loss_func\n",
    "\n",
    "    def __call__(self, x, context):\n",
    "        e = self.embed(context)\n",
    "        shape = e.shape\n",
    "        x = F.broadcast_to(x[:, None], (shape[0], shape[1]))\n",
    "        e = F.reshape(e, (shape[0] * shape[1], shape[2]))\n",
    "        x = F.reshape(x, (shape[0] * shape[1],))\n",
    "        loss = self.loss_func(e, x)\n",
    "        reporter.report({'loss': loss}, self)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call the constructor `__init__`, we pass the vocabulary size `n_vocab`, the size of the embedding vector `n_units`, and the loss function `loss_func` as arguments.\n",
    "\n",
    "The `Parameter`s are initialized in `self.init_scope()` It is recommended to initialize :class:`~chaier.Parameter` here. Since we set :class:`~chaier.Parameter` as the attribute of Link, there are effects such as making IDE easier to follow code.\n",
    "\n",
    "For details, see [New-style parameter registration APIs are added to Link](https://docs.chainer.org/en/latest/upgrade.html#new-style-parameter-registration-apis-are-added-to-link).\n",
    "\n",
    "The weight matrix `self.embed.W` is the embbeding matrix for input :math:`W_H`. The function call `__call__` takes Center Word's ID `x` and Context Word's ID `contexts` as arguments, and returns the error calculated by the loss function `self.loss_func`. When the function `__call__` is called, the shape of `x` is `[batch_size,]` and the shape of `contexts` is `[batch_size, n_context]`. The `batch_size` means the size of mini-batch, and `n_context` means the size of Context Words.\n",
    "\n",
    "First, we obtain the embedding vectors of `contexts` by `e = self.embed(contexts)`. In the Skip-gram, since each Center Word has only one Context Word, there is no problem to switch Context Word and Center Word. So, in the code, Context Word is used as input for the network. (This is because it is easy to match the CBoW code.)\n",
    "\n",
    "By `F.broadcast_to(x[:, None], (shape[0], shape[1]))`, the Center Word's ID `x` is broadcasted to each Context Word.\n",
    "\n",
    "At the end, the shape of `x` is `[batch_size * n_context,]` and the shape of `e` is `[batch_size * n_context, n_units]`. By `self.loss_func(e, x)`, the error is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Function\n",
    "\n",
    "Next, we define the loss function. Actually, this code also includes the part of the network structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropyLoss(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(SoftmaxCrossEntropyLoss, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.out = L.Linear(n_in, n_out, initialW=0)\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        return F.softmax_cross_entropy(self.out(x), t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing the linear transformation `self.out(x)`, which is defined by `L.Linear(n_in, n_out, initialW=0)`, we calculate the error function of cross entropy followed by softmax function with `softmax_cross_entropy`.\n",
    "\n",
    "Here, the linear transformation matrix `self.out.W` corresponds to the embedding matrix for output $W_O$, and `softmax_cross_entropy` corresponds to the softmax function and the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Iterator for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowIterator(chainer.dataset.Iterator):\n",
    "\n",
    "    def __init__(self, dataset, window, batch_size, repeat=True):\n",
    "        self.dataset = np.array(dataset, np.int32)\n",
    "        self.window = window\n",
    "        self.batch_size = batch_size\n",
    "        self._repeat = repeat\n",
    "\n",
    "        self.order = np.random.permutation(\n",
    "            len(dataset) - window * 2).astype(np.int32)\n",
    "        self.order += window\n",
    "        self.current_position = 0\n",
    "        self.epoch = 0\n",
    "        self.is_new_epoch = False\n",
    "\n",
    "    def __next__(self):\n",
    "        if not self._repeat and self.epoch > 0:\n",
    "            raise StopIteration\n",
    "\n",
    "        i = self.current_position\n",
    "        i_end = i + self.batch_size\n",
    "        position = self.order[i: i_end]\n",
    "        w = np.random.randint(self.window - 1) + 1\n",
    "        offset = np.concatenate([np.arange(-w, 0), np.arange(1, w + 1)])\n",
    "        pos = position[:, None] + offset[None, :]\n",
    "        context = self.dataset.take(pos)\n",
    "        center = self.dataset.take(position)\n",
    "\n",
    "        if i_end >= len(self.order):\n",
    "            np.random.shuffle(self.order)\n",
    "            self.epoch += 1\n",
    "            self.is_new_epoch = True\n",
    "            self.current_position = 0\n",
    "        else:\n",
    "            self.is_new_epoch = False\n",
    "            self.current_position = i_end\n",
    "\n",
    "        return center, context\n",
    "\n",
    "    @property\n",
    "    def epoch_detail(self):\n",
    "        return self.epoch + float(self.current_position) / len(self.order)\n",
    "\n",
    "    def serialize(self, serializer):\n",
    "        self.current_position = serializer('current_position',\n",
    "                                           self.current_position)\n",
    "        self.epoch = serializer('epoch', self.epoch)\n",
    "        self.is_new_epoch = serializer('is_new_epoch', self.is_new_epoch)\n",
    "        if self._order is not None:\n",
    "            serializer('_order', self._order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The constructor `__init__` receives the document dataset `dataset` as a list of word IDs, the window size `window` and the mini batch size `batch_size`.\n",
    "    - In the constructor, we create an array `self.order` which is shuffled `[window, window + 1, ..., len(dataset) - window - 1]` in order to iterate randomly ordered `dataset`.\n",
    "    - e.g. if the number of words in `dataset` is 100 and the window size `window` is 5, `self.order` becomes `numpy.ndarray` where numbers from 5 to 94 are shuffled.\n",
    "- The iterator definition `__next__` returns mini batch sized Center Word `center` and Context Word `contexts` according to the parameters of the constructor.\n",
    "    - The code `self.order[i:i_end]` generates the indices `position` of Center Words, which size is `batch_size`, from the random-ordered array `self.order`. The indices `position` will be converted to Center Words `center` by `self.dataset.take`.\n",
    "    - The code `np.concatenate([np.arange (-w, 0), np.arange(1, w + 1)])` creates the window offset `offset`.\n",
    "    - The code `position[:, None] + offset[None, :]` generates the indices of Context Words `pos` for each Center Word. The indices `pos` will be converted to Context Words `contexts` by `self.dataset.take`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, _ = chainer.datasets.get_ptb_words()\n",
    "counts = collections.Counter(train)\n",
    "counts.update(collections.Counter(val))\n",
    "n_vocab = max(train) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `train` and `val` means training data and validation data. Each data contains the list of Document IDs\n",
    "\n",
    "```\n",
    ">>> train\n",
    "array([ 0,  1,  2, ..., 39, 26, 24], dtype=int32)\n",
    ">>> val\n",
    "array([2211,  396, 1129, ...,  108,   27,   24], dtype=int32)\n",
    "```\n",
    "\n",
    "- The maximum id in `train` will be the vocabulary size `n_vocab - 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = 100\n",
    "loss_func = SoftmaxCrossEntropyLoss(unit, n_vocab)\n",
    "model = SkipGram(n_vocab, unit, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the `model` as Skip-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the loss function `loss_func` as `SoftmaxCrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = O.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the `optimizer` as Adam (Adaptive moment estimation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "batchsize = 1000\n",
    "gpu = -1\n",
    "train_iter = WindowIterator(train, window, batchsize)\n",
    "val_iter = WindowIterator(val, window, batchsize, repeat=False)\n",
    "\n",
    "updater = training.StandardUpdater(\n",
    "    train_iter, optimizer, device=gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the iterators and updater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "trainer = training.Trainer(\n",
    "    updater, (epoch, 'epoch'), out='word2vec_result')\n",
    "trainer.extend(extensions.Evaluator(\n",
    "    val_iter, model, device=gpu))\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'main/loss', 'validation/main/loss']))\n",
    "trainer.extend(extensions.ProgressBar())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the trainer and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in main training loop: all the input arrays must have same number of dimensions\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shunta/lib/chainer/chainer/training/trainer.py\", line 310, in run\n",
      "    update()\n",
      "  File \"/Users/shunta/lib/chainer/chainer/training/updater.py\", line 223, in update\n",
      "    self.update_core()\n",
      "  File \"/Users/shunta/lib/chainer/chainer/training/updater.py\", line 228, in update_core\n",
      "    in_arrays = self.converter(batch, self.device)\n",
      "  File \"/Users/shunta/lib/chainer/chainer/dataset/convert.py\", line 109, in concat_examples\n",
      "    return to_device(device, _concat_arrays(batch, padding))\n",
      "  File \"/Users/shunta/lib/chainer/chainer/dataset/convert.py\", line 123, in _concat_arrays\n",
      "    return xp.concatenate([array[None] for array in arrays])\n",
      "Will finalize trainer extensions and updater before reraising the exception.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3287ea1d3afc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/lib/chainer/chainer/training/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 print('Will finalize trainer extensions and updater before '\n\u001b[1;32m    323\u001b[0m                       'reraising the exception.', file=sys.stderr)\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.2/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/chainer/chainer/training/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/chainer/chainer/training/updater.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \"\"\"\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/chainer/chainer/training/updater.py\u001b[0m in \u001b[0;36mupdate_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0min_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/chainer/chainer/dataset/convert.py\u001b[0m in \u001b[0;36mconcat_examples\u001b[0;34m(batch, device, padding)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_concat_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lib/chainer/chainer/dataset/convert.py\u001b[0m in \u001b[0;36m_concat_arrays\u001b[0;34m(arrays, padding)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mxp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_array_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_from_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
