{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# How to write a training loop in Chainer\n",
    "\n",
    "In this tutorial section, we will learn how to train a deep neural network to classify images of hand-written digits in the popular MNIST dataset. This dataset contains 50,000 training examples and 10,000 test examples. Each example is a set of a 28 x 28 greyscale image and a corresponding class label. Since the digits from 0 to 9 are used, there are 10 classes for the labels.\n",
    "\n",
    "Chainer provides a feature called [Trainer](https://docs.chainer.org/en/latest/reference/core/generated/chainer.training.Trainer.html#chainer.training.Trainer) that can simplify the training procedure of your model. However, it is also good to know how the training works in Chainer before starting to use the useful [Trainer](https://docs.chainer.org/en/latest/reference/core/generated/chainer.training.Trainer.html#chainer.training.Trainer) class that hides the actual processes. Writing your own training loop can be useful for learning how [Trainer](https://docs.chainer.org/en/latest/reference/core/generated/chainer.training.Trainer.html#chainer.training.Trainer) works or for implementing features not included in the standard trainer.\n",
    "\n",
    "The complete training procedure consists of the following steps:\n",
    "\n",
    "1. Prepare a dataset\n",
    "2. Create a dataset iterator\n",
    "3. Define a network\n",
    "4. Select an optimization algorithm\n",
    "5. Write a training loop\n",
    "    1. Retrieve a set of examples (mini-batch) from the training dataset.\n",
    "    2. Feed the mini-batch to your network.\n",
    "    3. Run a forward pass of the network and compute the loss.\n",
    "    4. Just call the [backward()](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Variable.html#chainer.Variable.backward) method from the loss [Variable](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Variable.html#chainer.Variable) to compute the gradients for all trainable parameters.\n",
    "    5. Run the optimizer to update those parameters.\n",
    "6. Save the trained model\n",
    "7. Perform classification by the saved model and check the network performance on validation/test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, let's import the necessary packages for using Chainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "import matplotlib.pyplot as plt\n",
    "from chainer.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Prepare a dataset\n",
    "\n",
    "Chainer contains some built-in functions to use some popular datasets like MNIST, CIFAR10/100, etc. Those can automatically download the data from servers and provide dataset objects which are easy to use.\n",
    "\n",
    "The code below shows how to retrieve the MNIST dataset from the server and save an image from its training split to make sure the images are correctly obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Download the MNIST data if you haven't downloaded it yet\n",
    "train, test = mnist.get_mnist(withlabel=True, ndim=1)\n",
    "\n",
    "# Display an example from the MNIST dataset.\n",
    "# `x` contains the inpu t image array and `t` contains that target class\n",
    "# label as an integer.\n",
    "x, t = train[0]\n",
    "plt.imshow(x.reshape(28, 28), cmap='gray')\n",
    "plt.savefig('5.png')\n",
    "print('label:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The saved image `5.png` will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('5.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Create a dataset iterator\n",
    "\n",
    "Although this is an optional step, we’d like to introduce the [Iterator](https://docs.chainer.org/en/latest/reference/core/generated/chainer.dataset.Iterator.html#chainer.dataset.Iterator) class that retrieves a set of data and labels from the given dataset to easily make a mini-batch. There are some subclasses that can perform the same thing in different ways, e.g., using multi-processing to parallelize the data loading part, etc.\n",
    "\n",
    "Here, we use [SerialIterator](https://docs.chainer.org/en/latest/reference/generated/chainer.iterators.SerialIterator.html#chainer.iterators.SerialIterator), which is also a subclass of [Iterator](https://docs.chainer.org/en/latest/reference/core/generated/chainer.dataset.Iterator.html#chainer.dataset.Iterator) in the example code below. The [SerialIterator](https://docs.chainer.org/en/latest/reference/generated/chainer.iterators.SerialIterator.html#chainer.iterators.SerialIterator) can provide mini-batches with or without shuffling the order of data in the given dataset.\n",
    "\n",
    "All [Iterators](https://docs.chainer.org/en/latest/reference/core/generated/chainer.dataset.Iterator.html#chainer.dataset.Iterator) produce a new mini-batch by calling its [next()](https://docs.chainer.org/en/latest/reference/core/generated/chainer.dataset.Iterator.html#chainer.dataset.Iterator.next) method. All [Iterators](https://docs.chainer.org/en/latest/reference/core/generated/chainer.dataset.Iterator.html#chainer.dataset.Iterator) also have properties to know how many times we have taken all the data from the given dataset (epoch) and whether the next mini-batch will be the start of a new epoch (`is_new_epoch`), and so on.\n",
    "\n",
    "The code below shows how to create a [SerialIterator](https://docs.chainer.org/en/latest/reference/generated/chainer.iterators.SerialIterator.html#chainer.iterators.SerialIterator) object from a dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from chainer import iterators\n",
    "\n",
    "# Choose the minibatch size.\n",
    "batchsize = 128\n",
    "\n",
    "train_iter = iterators.SerialIterator(train, batchsize)\n",
    "test_iter = iterators.SerialIterator(\n",
    "    test, batchsize, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note**\n",
    "\n",
    "`iterator`s can take a built-in Python list as a given dataset. It means that the example code below is able to work,\n",
    "\n",
    "```\n",
    "train = [(x1, t1), (x2, t2), ...]  # A list of tuples\n",
    "train_iter = iterators.SerialIterator(train, batchsize)\n",
    "```\n",
    "\n",
    "where `x1, x2, ...` denote the input data and `t1, t2, ...` denote the corresponding labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Details of [SerialIterator](https://docs.chainer.org/en/latest/reference/generated/chainer.iterators.SerialIterator.html#chainer.iterators.SerialIterator)\n",
    "\n",
    "- [SerialIterator](https://docs.chainer.org/en/latest/reference/generated/chainer.iterators.SerialIterator.html#chainer.iterators.SerialIterator) is a built-in subclass of [Iterator](https://docs.chainer.org/en/latest/reference/core/generated/chainer.dataset.Iterator.html#chainer.dataset.Iterator) that can retrieve a mini-batch from a given dataset in either sequential or shuffled order.\n",
    "- The [Iterator](https://docs.chainer.org/en/latest/reference/core/generated/chainer.dataset.Iterator.html#chainer.dataset.Iterator)‘s constructor takes two arguments: a dataset object and a mini-batch size.\n",
    "- If you want to use the same dataset repeatedly during the training process, set the `repeat` argument to `True` (default). Otherwise, the dataset will be used only one time. The latter case is actually for the evaluation.\n",
    "- If you want to shuffle the training dataset every epoch, set the `shuffle` argument to `True`. Otherwise, the order of each data retrieved from the dataset will be always the same at each epoch.\n",
    "\n",
    "In the example code shown above, we set `batchsize = 128` in both `train_iter` and `test_iter`. So, these iterators will provide 128 images and corresponding labels at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Define a network\n",
    "\n",
    "Now let’s define a neural network that we will train to classify the MNIST images. For simplicity, we use a three-layer perceptron here. We set each hidden layer to have 100 units and set the output layer to have 10 units, which is corresponding to the number of class labels of the MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create your network as a subclass of Chain\n",
    "\n",
    "You can create your network by writing a new subclass of [Chain](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Chain.html#chainer.Chain). The main steps are twofold:\n",
    "\n",
    "1. Register the network components which have trainable parameters to the subclass. Each of them must be instantiated and assigned to a property in the scope specified by [init_scope()](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Chain.html#chainer.Chain.init_scope)\n",
    "2. Define a `__call__()` method that represents the actual **forward computation** of your network. This method takes one or more [Variable](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Variable.html#chainer.Variable), `numpy.array`, or `cupy.array` as its inputs and calculates the forward pass using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MyNetwork(Chain):\n",
    "\n",
    "    def __init__(self, n_mid_units=100, n_out=10):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_mid_units)\n",
    "            self.l2 = L.Linear(n_mid_units, n_mid_units)\n",
    "            self.l3 = L.Linear(n_mid_units, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.l1(x))\n",
    "        h = F.relu(self.l2(h))\n",
    "        return self.l3(h)\n",
    "\n",
    "model = MyNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[Link](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Link.html#chainer.Link), [Chain](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Chain.html#chainer.Chain), [ChainList](https://docs.chainer.org/en/latest/reference/core/generated/chainer.ChainList.html#chainer.ChainList), and those subclass objects which contain trainable parameters should be registered to the model by assigning it as a property inside the [init_scope()](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Chain.html#chainer.Chain.init_scope). For example, a [Function](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Function.html#chainer.Function) does not contain any trainable parameters, so there is no need to keep the object as a property of your network. When you want to use [relu()](https://docs.chainer.org/en/latest/reference/generated/chainer.functions.relu.html#chainer.functions.relu) in your network, using it as a function in `__call__()` works correctly.\n",
    "\n",
    "In Chainer, the Python code that implements the forward computation itself represents the network. In other words, we can conceptually think of the computation graph for our network being constructed dynamically as this forward computation code executes. This allows Chainer to describe networks in which different computations can be performed in each iteration, such as branched networks, intuitively and with a high degree of flexibility. This is the key feature of Chainer that we call **Define-by-Run**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. Select an optimization algorithm\n",
    "\n",
    "Chainer provides a wide variety of optimization algorithms that can be used to optimize the network parameters during training. They are located in `optimizers` module.\n",
    "\n",
    "Here, we are going to use the stochastic gradient descent (SGD) method with momentum, which is implemented by [MomentumSGD](https://docs.chainer.org/en/latest/reference/generated/chainer.optimizers.MomentumSGD.html#chainer.optimizers.MomentumSGD). To use the optimizer, we give the network object (typically it’s a [Chain](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Chain.html#chainer.Chain) or [ChainList](https://docs.chainer.org/en/latest/reference/core/generated/chainer.ChainList.html#chainer.ChainList)) to the [setup()](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Optimizer.html#chainer.Optimizer.setup) method of the optimizer object to register it. In this way, the [Optimizer](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Optimizer.html#chainer.Optimizer) can automatically find the model parameters and update them during training.\n",
    "\n",
    "You can easily try out other optimizers as well. Please test and observe the results of various optimizers. For example, you could try to change [MomentumSGD](https://docs.chainer.org/en/latest/reference/generated/chainer.optimizers.MomentumSGD.html#chainer.optimizers.MomentumSGD) to [Adam](https://docs.chainer.org/en/latest/reference/generated/chainer.optimizers.Adam.html#chainer.optimizers.Adam), [RMSprop](https://docs.chainer.org/en/latest/reference/generated/chainer.optimizers.RMSprop.html#chainer.optimizers.RMSprop), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Choose an optimizer algorithm\n",
    "optimizer = optimizers.MomentumSGD(lr=0.01, momentum=0.9)\n",
    "\n",
    "# Give the optimizer a reference to the model so that it\n",
    "# can locate the model's parameters.\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note**\n",
    "\n",
    "In the above example, we set `lr` to 0.01 in the constructor. This value is known as the “learning rate”, one of the most important hyperparameters that need to be adjusted in order to obtain the best performance. The various optimizers may each have different hyperparameters and so be sure to check the documentation for the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. Write a training loop\n",
    "\n",
    "We now show how to write the training loop. Since we are working on a digit classification problem, we will use [softmax_cross_entropy()](https://docs.chainer.org/en/latest/reference/generated/chainer.functions.softmax_cross_entropy.html#chainer.functions.softmax_cross_entropy) as the loss function for the optimizer to minimize. For other types of problems, such as regression models, other loss functions might be more appropriate. See the [Chainer documentation for detailed information on the various loss functions](http://docs.chainer.org/en/stable/reference/functions.html#loss-functions) for more details.\n",
    "\n",
    "Our training loop will be structured as follows.\n",
    "\n",
    "1. We will first get a mini-batch of examples from the training dataset.\n",
    "2. We will then feed the batch into our network by calling it (a [Chain](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Chain.html#chainer.Chain) object) like a function. This will execute the forward-pass code that are written in the `__call__()` method.\n",
    "3. This will return the network output that represents class label predictions. We supply it to the loss function along with the true (that is, target) values. The loss function will output the loss as a [Variable](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Variable.html#chainer.Variable) object.\n",
    "4. We then clear any previous gradients in the network and perform the backward pass by calling the [backward()](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Variable.html#chainer.Variable.backward) method on the loss variable which computes the parameter gradients. We need to clear the gradients first because the [backward()](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Variable.html#chainer.Variable.backward) method accumulates gradients instead of overwriting the previous values.\n",
    "5. Since the optimizer already has a reference to the network, it has access to the parameters and the computed gradients so that we can now call the [update()](https://docs.chainer.org/en/latest/reference/core/generated/chainer.Optimizer.html#chainer.Optimizer.update) method of the optimizer which will update the model parameters.\n",
    "\n",
    "In addition to the above steps, you might want to check the performance of the network with a validation dataset. This allows you to observe how well it is generalized to new data so far, namely, you can check whether it is overfitting to the training data. The code below checks the performance on the test set at the end of each epoch. The code has the same structure as the training code except that no backpropagation is performed and we also compute the accuracy on the test data using the [accuracy()](https://docs.chainer.org/en/latest/reference/generated/chainer.functions.accuracy.html#chainer.functions.accuracy) function.\n",
    "\n",
    "The training loop code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer.dataset import concat_examples\n",
    "from chainer.cuda import to_cpu\n",
    "\n",
    "max_epoch = 10\n",
    "gpu_id = 0  # If you want to use GPU, set 0 (GPU ID you want to use)\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    model.to_gpu(gpu_id)\n",
    "\n",
    "while train_iter.epoch < max_epoch:\n",
    "\n",
    "    # ---------- One iteration of the training loop ----------\n",
    "    train_batch = train_iter.next()\n",
    "    image_train, target_train = concat_examples(train_batch, gpu_id)\n",
    "\n",
    "    # Calculate the prediction of the network\n",
    "    prediction_train = model(image_train)\n",
    "\n",
    "    # Calculate the loss with softmax_cross_entropy\n",
    "    loss = F.softmax_cross_entropy(prediction_train, target_train)\n",
    "\n",
    "    # Calculate the gradients in the network\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update all the trainable paremters\n",
    "    optimizer.update()\n",
    "    # --------------------- until here ---------------------\n",
    "\n",
    "    # Check the validation accuracy of prediction after every epoch\n",
    "    if train_iter.is_new_epoch:  # If this iteration is the final iteration of the current epoch\n",
    "\n",
    "        # Display the training loss\n",
    "        print('epoch:{:02d} train_loss:{:.04f} '.format(\n",
    "            train_iter.epoch, float(to_cpu(loss.data))), end='')\n",
    "\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        while True:\n",
    "            test_batch = test_iter.next()\n",
    "            image_test, target_test = concat_examples(test_batch, gpu_id)\n",
    "\n",
    "            # Forward the test data\n",
    "            prediction_test = model(image_test)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss_test = F.softmax_cross_entropy(prediction_test, target_test)\n",
    "            test_losses.append(to_cpu(loss_test.data))\n",
    "\n",
    "            # Calculate the accuracy\n",
    "            accuracy = F.accuracy(prediction_test, target_test)\n",
    "            accuracy.to_cpu()\n",
    "            test_accuracies.append(accuracy.data)\n",
    "\n",
    "            if test_iter.is_new_epoch:\n",
    "                test_iter.reset()\n",
    "                break\n",
    "\n",
    "        print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
    "            np.mean(test_losses), np.mean(test_accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. Save the trained model\n",
    "\n",
    "Chainer provides two types of [serializers](https://docs.chainer.org/en/latest/reference/serializers.html#module-chainer.serializers) that can be used to save and restore model state. One supports the HDF5 format and the other supports the NumPy NPZ format. For this example, we are going to use the NPZ format to save our model since it is easy to use with NumPy and doesn’t need to install any additional dependencies or libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "serializers.save_npz('my_mnist.model', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 7. Perform classification by the saved model\n",
    "\n",
    "Let’s use the saved model to classify a new image. In order to load the trained model parameters, we need to perform the following two steps:\n",
    "\n",
    "1. Instantiate the same network as what you trained.\n",
    "2. Overwrite all parameters in the model instance with the saved weights using the [load_npz()](https://docs.chainer.org/en/latest/reference/generated/chainer.serializers.load_npz.html#chainer.serializers.load_npz) function.\n",
    "\n",
    "Once the model is restored, it can be used to predict image labels on new input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from chainer import serializers\n",
    "\n",
    "# Create an instance of the network you trained\n",
    "model = MyNetwork()\n",
    "\n",
    "# Load the saved paremeters into the instance\n",
    "serializers.load_npz('my_mnist.model', model)\n",
    "\n",
    "# Get a test image and label\n",
    "x, t = test[0]\n",
    "plt.imshow(x.reshape(28, 28), cmap='gray')\n",
    "plt.savefig('7.png')\n",
    "print('label:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The saved test image looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Image('7.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Change the shape of the minibatch.\n",
    "# In this example, the size of minibatch is 1.\n",
    "# Inference using any mini-batch size can be performed.\n",
    "\n",
    "print(x.shape, end=' -> ')\n",
    "x = x[None, ...]\n",
    "print(x.shape)\n",
    "\n",
    "# forward calculation of the model by sending X\n",
    "y = model(x)\n",
    "\n",
    "# The result is given as Variable, then we can take a look at the contents by the attribute, .data.\n",
    "y = y.data\n",
    "\n",
    "# Look up the most probable digit number using argmax\n",
    "pred_label = y.argmax(axis=1)\n",
    "\n",
    "print('predicted label:', pred_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**The prediction result looks correct. Yay!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
