{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Write an RNN Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "0. Introduction\n",
    "================\n",
    "\n",
    "The **language model** is modeling the probability of generating natural language sentences or documents. You can use the language model to estimate how natural a sentence or a document is. Also, with the language model, you can generate new sentences or documents.\n",
    "\n",
    "Let's start with modeling the probability of generating sentences. We represent a sentence as ${\\bf X} = ({\\bf x}_0, {\\bf x}_1, \\dots, {\\bf x}_T)$, in which ${\\bf x}_t$ is a one-hot vector. Generally, ${\\bf x}_0$ is the one-hot vector of **BOS** (beginning of sentence), and ${\\bf x}_T$ is that of **EOS** (end of sentence).\n",
    "\n",
    "A language model models the probability of a word occurance under the condition of its previous words in a sentence. Let ${\\bf X}_{[i, j]}$ be $({\\bf x}_i, {\\bf x}_{i+1}, \\dots, {\\bf x}_j)$ , the occurrence probability of sentence ${\\bf X}$ can be represented as follows:\n",
    "\n",
    "$$P({\\bf X}) = P({\\bf x}_0) \\prod_{t=1}^T P({\\bf x}_t \\mid {\\bf X}_{[0, t-1]})$$\n",
    "\n",
    "So, the language model $P({\\bf X})$ can be decomposed into word probabilities conditioned with its previous words.\n",
    "\n",
    "In this notebook, we model $P({\\bf x}_t \\mid {\\bf X}_{[0, t-1]})$ with a recurrent neural network to obtain a language model $P({\\bf X})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. Basic Idea of Recurrent Neural Net Language Model\n",
    "=====================================================\n",
    "\n",
    "1.1 Recurrent Neural Net Language Model\n",
    "---------------------------------------\n",
    "\n",
    "**Recurrent Neurral Net Language Model** (RNNLM) is a type of neural net language models which contains RNNs in the network. Since an RNN can deal with the variable length inputs, it is suitable for modeling sequential data such as sentences in natural language.\n",
    "\n",
    "We show one layer of a RNNLM with these parameters.\n",
    "\n",
    "| Symbol | Definition |\n",
    "|-------:|:-----------|\n",
    "| ${\\bf x}_t$ | the one-hot vector of $t$-th word |\n",
    "| ${\\bf y}_t$ | the $t$-th output |\n",
    "| ${\\bf h}_t^{(i)}$ | the $t$-th hidden layer of $i$-th layer |\n",
    "| ${\\bf p}_t$ | the next word's probability of $t$-th word |\n",
    "| ${\\bf E}$ | Embedding matrix |\n",
    "| ${\\bf W}_h$ | Hidden layer matrix |\n",
    "| ${\\bf W}_o$ | Output layer matrix |\n",
    "\n",
    "\n",
    "![rnnlm](rnnlm.png)\n",
    "\n",
    "**The process to get a next word prediction from $t$-th input word ${\\bf x}_t$**\n",
    "\n",
    "1. Get the embedding vector: ${\\bf h}_t^{(0)} = {\\bf E} {\\bf x}_t$\n",
    "2. Calculate the hidden layer: ${\\bf h}_t^{(1)} = {\\rm tanh}\\left({\\bf W}_h \\left[ \\begin{array}{cc} {\\bf h}_t^{(0)} \\\\ {\\bf h}_{t-1}^{(1)} \\end{array} \\right]\\right)$\n",
    "3. Calculate the output layer: ${\\bf y}_t = {\\bf W}_o {\\bf h}_t^{(1)}$\n",
    "4. Transform to probability: ${\\bf p}_t = {\\rm softmax}({\\bf y}_t)$\n",
    "\n",
    "- Note that ${\\rm tanh}$ in the above equation is applied to the input vector in element-wise manner.\n",
    "- Note that $\\left[ \\begin{array}{cc} {\\bf a} \\\\ {\\bf b} \\end{array} \\right]$ denotes a concatenated vector of ${\\bf a}$ and ${\\bf b}$.\n",
    "- Note that ${\\rm softmax}$ in the above equation converts an arbitrary real vector to a probability vector which the summation over all elements is $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1.2 Perplexity (Evaluation of the language model)\n",
    "-----------------------------------------------\n",
    "\n",
    "**Perplexity** is the common evaluation metric for a language model. Generally, it measures how well the proposed probability model $P_{\\rm model}({\\bf X})$ represents the target data $P^*({\\bf X})$.\n",
    "\n",
    "Let a validation dataset be $D = \\{{\\bf X}^{(n)}\\}_{n=1}^{|D|}$, which is a set of sentences, where the $n$-th sentence length is $T^{(n)}$, and the vocabulary size of this dataset is $|\\mathcal{V}|$, the perplexity is represented as follows:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "&& b^z \\\\\n",
    "&& s.t.~ ~ ~ z = - \\frac{1}{|\\mathcal{V}|}\n",
    "\\sum_{n=1}^{|D|} \\sum_{t=1}^{T^{(n)}} \\log_b P_{\\rm model}({\\bf x}_t^{(n)}, {\\bf X}_{[a, t-1]}^{(n)})\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We usually use $b = 2$ or $b = e$. The perplexity shows how much varied the predicted distribution for the next word is. When a language model well represents the dataset, it should show a high probability only for the correct next word, so that the entropy should be high. In the above equation, the sign is reversed, so that smaller perplexity means better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we minimize the below cross entropy:\n",
    "\n",
    "$$\n",
    "\\mathcal{H}(\\hat{P}, P_{\\rm model}) = - \\hat{P}({\\bf X}) \\log P_{\\rm model}({\\bf X})\n",
    "$$\n",
    "\n",
    "where $\\hat P$ is the empirical distribution of a sequence in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2. Implementation of Recurrent Neural Net Language Model\n",
    "=========================================================\n",
    "\n",
    "**There is an example of RNN language model in the official repository, so we will explain how to implement a RNNLM in Chainer based on that: [chainer/examples/ptb](https://github.com/chainer/chainer/tree/master/examples/ptb)**\n",
    "\n",
    "2.1 Model overview\n",
    "-----------------\n",
    "\n",
    "![rnnlm_example](rnnlm_example.png)\n",
    "\n",
    "The RNNLM used in this notebook is depicted in the above figure. The symbols appeared in the figure are defined as follows:\n",
    "\n",
    "| Symbol | Definition |\n",
    "|-------:|:-----------|\n",
    "| ${\\bf x}_t$ | the one-hot vector of $t$-th input |\n",
    "| ${\\bf y}_t$ | the $t$-th output |\n",
    "| ${\\bf h}_t^{(i)}$ | the $t$-th hidden vector of $i$-th layer |\n",
    "| ${\\bf E}$ | Embedding matrix |\n",
    "| ${\\bf W}_o$ | Output layer matrix |\n",
    "\n",
    "**LSTMs** (long short-term memory) are used for the connection of hidden layers. A LSTM is one of major recurrent neural net modules. It is desined for remembering the long-term memory, so that it should be able to consider relationships of distant words, such that a word at beginning of sentence and it at the end. We also use **Dropout** before both LSTMs and linear transformations. Dropout is one of regularization techniques for preventing overfitting on training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2.2 Step-by-step implementation\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.1 Import Package\n",
    "\n",
    "First, let's import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.2 Define training settings\n",
    "\n",
    "Define all training settings here for ease of reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batchsize = 20\n",
    "bproplen = 35\n",
    "epoch = 39\n",
    "gpu = 0  # negative value to run in CPU\n",
    "gradclip = 5\n",
    "unit = 650\n",
    "interval = 500\n",
    "update_interval = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.3 Define Network Structure\n",
    "\n",
    "An RNNLM written in Chainer is shown below. It implements the model depicted in the above figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RNNLM(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_vocab, n_units):\n",
    "        super(RNNLM, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.embed = L.EmbedID(n_vocab, n_units)\n",
    "            self.l1 = L.LSTM(n_units, n_units)\n",
    "            self.l2 = L.LSTM(n_units, n_units)\n",
    "            self.l3 = L.Linear(n_units, n_vocab)\n",
    "\n",
    "        for param in self.params():\n",
    "            param.data[...] = np.random.uniform(-0.1, 0.1, param.data.shape)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l1.reset_state()\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h0 = self.embed(x)\n",
    "        h1 = self.l1(F.dropout(h0))\n",
    "        h2 = self.l2(F.dropout(h1))\n",
    "        y = self.l3(F.dropout(h2))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we insatantiate this class for making a model, we give the vocavulary size to `n_vocab` and the size of hidden vectors to `n_units`.\n",
    "- This network uses `chainer.links.LSTM`, `chainer.links.Linear`, and `chainer.functions.dropout` as its building blocks.\n",
    "- All the layers are registered and initialized in the context with `self.init_scope()`.\n",
    "- You can access all the parameters in those layers by calling `self.params()`.\n",
    "- In the constructor, it initializes all parameters with values sampled from a uniform distribution $U(-1, 1)$.\n",
    "- The `__call__` method takes an word ID `x`, and calculates the word probability vector for the next word by forwarding it through the nerwork, and returns the output.\n",
    "- Note that the word ID `x` is automatically converted to a $|\\mathcal{V}|$-dimensional one-hot vector and then multiplied with the input embedding matrix in `self.embed(x)` to obtain an embed vector `h0` at the first line of `__call__`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Load the Penn Tree Bank long word sequence dataset\n",
    "\n",
    "In this notebook, we use [Penn Tree Bank](https://www.cis.upenn.edu/~treebank/) dataset that contains number of sentences. Chainer provides an utility function to obtain this dataset from server and convert it to a long single sequence of word IDs. `chainer.datasets.get_ptb_words()` actually returns three separated datasets which are for train, validation, and test.\n",
    "\n",
    "Let's download and make dataset objects using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, val, test = chainer.datasets.get_ptb_words()\n",
    "n_vocab = max(train) + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.5 Define Iterator for making a mini-batch from the dataset\n",
    "\n",
    "Dataset iterator creates a mini-batch of couple of words at different positions, namely, pairs of current word and its next word. Each example is a part of sentences starting from different offsets equally spaced within the whole sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ParallelSequentialIterator(chainer.dataset.Iterator):\n",
    "\n",
    "    def __init__(self, dataset, batch_size, repeat=True):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        # batch size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Number of completed sweeps over the dataset. In this case, it is\n",
    "        # incremented if every word is visited at least once after the last\n",
    "        # increment.\n",
    "        self.epoch = 0\n",
    "        \n",
    "        # True if the epoch is incremented at the last iteration.\n",
    "        self.is_new_epoch = False\n",
    "        self.repeat = repeat\n",
    "        length = len(dataset)\n",
    "        \n",
    "        # Offsets maintain the position of each sequence in the mini-batch.\n",
    "        self.offsets = [i * length // batch_size for i in range(batch_size)]\n",
    "        \n",
    "        # NOTE: this is not a count of parameter updates. It is just a count of\n",
    "        # calls of `__next__`.\n",
    "        self.iteration = 0\n",
    "        \n",
    "        # use -1 instead of None internally\n",
    "        self._previous_epoch_detail = -1.\n",
    "\n",
    "    def __next__(self):\n",
    "        # This iterator returns a list representing a mini-batch. Each item\n",
    "        # indicates a different position in the original sequence. Each item is\n",
    "        # represented by a pair of two word IDs. The first word is at the\n",
    "        # \"current\" position, while the second word is at the next position.\n",
    "        # At each iteration, the iteration count is incremented, which pushes\n",
    "        # forward the \"current\" position.\n",
    "        length = len(self.dataset)\n",
    "        if not self.repeat and self.iteration * self.batch_size >= length:\n",
    "            # If not self.repeat, this iterator stops at the end of the first\n",
    "            # epoch (i.e., when all words are visited once).\n",
    "            raise StopIteration\n",
    "        cur_words = self.get_words()\n",
    "        self._previous_epoch_detail = self.epoch_detail\n",
    "        self.iteration += 1\n",
    "        next_words = self.get_words()\n",
    "\n",
    "        epoch = self.iteration * self.batch_size // length\n",
    "        self.is_new_epoch = self.epoch < epoch\n",
    "        if self.is_new_epoch:\n",
    "            self.epoch = epoch\n",
    "\n",
    "        return list(zip(cur_words, next_words))\n",
    "\n",
    "    @property\n",
    "    def epoch_detail(self):\n",
    "        # Floating point version of epoch.\n",
    "        return self.iteration * self.batch_size / len(self.dataset)\n",
    "\n",
    "    @property\n",
    "    def previous_epoch_detail(self):\n",
    "        if self._previous_epoch_detail < 0:\n",
    "            return None\n",
    "        return self._previous_epoch_detail\n",
    "\n",
    "    def get_words(self):\n",
    "        # It returns a list of current words.\n",
    "        return [self.dataset[(offset + self.iteration) % len(self.dataset)]\n",
    "                for offset in self.offsets]\n",
    "\n",
    "    def serialize(self, serializer):\n",
    "        # It is important to serialize the state to be recovered on resume.\n",
    "        self.iteration = serializer('iteration', self.iteration)\n",
    "        self.epoch = serializer('epoch', self.epoch)\n",
    "        try:\n",
    "            self._previous_epoch_detail = serializer(\n",
    "                'previous_epoch_detail', self._previous_epoch_detail)\n",
    "        except KeyError:\n",
    "            # guess previous_epoch_detail for older version\n",
    "            self._previous_epoch_detail = self.epoch + \\\n",
    "                (self.current_position - self.batch_size) / len(self.dataset)\n",
    "            if self.epoch_detail > 0:\n",
    "                self._previous_epoch_detail = max(\n",
    "                    self._previous_epoch_detail, 0.)\n",
    "            else:\n",
    "                self._previous_epoch_detail = -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.6 Define Updater\n",
    "\n",
    "We use Backpropagation through time (BPTT) for optimize the RNNLM. BPTT can be implemented by overriding `update_core()` method of `StandardUpdater`. First, in the constructor of the `BPTTUpdater`, it takes `bprop_len` as an argument in addiotion to other arguments `StandardUpdater` needs. `bprop_len` defines the length of sequence $T$ to calculate the loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_{t=0}^T \\sum_{n=1}^{|\\mathcal{V}|}\n",
    "\\hat{P}({\\bf x}_{t+1}^{(n)})\n",
    "\\log\n",
    "P_{\\rm model}({\\bf x}_{t+1}^{(n)} \\mid {\\bf x}_t^{(n)})\n",
    "$$\n",
    "\n",
    "where $\\hat{P}({\\bf x}_t^n)$ is a probability for $n$-th word in the vocabulary at the position $t$ in the training data sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BPTTUpdater(training.StandardUpdater):\n",
    "\n",
    "    def __init__(self, train_iter, optimizer, bprop_len, device):\n",
    "        super(BPTTUpdater, self).__init__(\n",
    "            train_iter, optimizer, device=device)\n",
    "        self.bprop_len = bprop_len\n",
    "\n",
    "    # The core part of the update routine can be customized by overriding.\n",
    "    def update_core(self):\n",
    "        loss = 0\n",
    "        # When we pass one iterator and optimizer to StandardUpdater.__init__,\n",
    "        # they are automatically named 'main'.\n",
    "        train_iter = self.get_iterator('main')\n",
    "        optimizer = self.get_optimizer('main')\n",
    "\n",
    "        # Progress the dataset iterator for bprop_len words at each iteration.\n",
    "        for i in range(self.bprop_len):\n",
    "            # Get the next batch (a list of tuples of two word IDs)\n",
    "            batch = train_iter.__next__()\n",
    "\n",
    "            # Concatenate the word IDs to matrices and send them to the device\n",
    "            # self.converter does this job\n",
    "            # (it is chainer.dataset.concat_examples by default)\n",
    "            x, t = self.converter(batch, self.device)\n",
    "\n",
    "            # Compute the loss at this time step and accumulate it\n",
    "            loss += optimizer.target(chainer.Variable(x), chainer.Variable(t))\n",
    "\n",
    "        optimizer.target.cleargrads()  # Clear the parameter gradients\n",
    "        loss.backward()  # Backprop\n",
    "        loss.unchain_backward()  # Truncate the graph\n",
    "        optimizer.update()  # Update the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.7 Define Evaluation Function (Perplexity)\n",
    "\n",
    "Define a function to calculate the perplexity from the loss value. If we take $e$ as $b$ in the above definition of perplexity, calculating the perplexity is just to give the loss value to the power of $e$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(result):\n",
    "    result['perplexity'] = np.exp(result['main/loss'])\n",
    "    if 'validation/main/loss' in result:\n",
    "        result['val_perplexity'] = np.exp(result['validation/main/loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.8 Create iterators\n",
    "\n",
    "Here, the code below just create iterator objects from dataset splits (train/val/test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_iter = ParallelSequentialIterator(train, batchsize)\n",
    "val_iter = ParallelSequentialIterator(val, 1, repeat=False)\n",
    "test_iter = ParallelSequentialIterator(test, 1, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.9 Create RNN and classification model\n",
    "\n",
    "Instantiate RNNLM model and wrap it with `L.Classifier` because it calculates softmax cross entropy as the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rnn = RNNLM(n_vocab, unit)\n",
    "model = L.Classifier(rnn)\n",
    "model.compute_accuracy = False  # we only want the perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that `chainer.links.Classifier` computes not only the loss but also accuracy based on a given input/label pair. To learn the RNN language model, we only need the loss (cross entropy) in the `Classifier` because we calculate the perplexity instead of classification accuracy to check the performance of the model. So, we turn off computing the accuracy by giving `False` to `model.compute_accuracy` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.10 Setup optimizer\n",
    "\n",
    "Prepare an optimizer. Here, we use `GradientClipping` to prevent gradient explosion. It automatically clip the gradient to be used to update the parameters in the model with given constant `gradclip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "optimizer = chainer.optimizers.SGD(lr=1.0)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.GradientClipping(gradclip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.11 Setup and run trainer\n",
    "\n",
    "Let's make an `trainer` object and start the training! Note that we add an `eval_hook` to the `Evaluator` extension to reset the internal states before starting evaluation process. It can prevent to use training data during evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       iteration   perplexity  val_perplexity\n",
      "\u001b[J0           500         688.304                     \n",
      "\u001b[J0           1000        282.054                     \n",
      "\u001b[J1           1500        216.749     200.702         \n",
      "\u001b[J1           2000        200.301                     \n",
      "\u001b[J1           2500        166.433                     \n",
      "\u001b[J2           3000        152.462     150.148         \n",
      "\u001b[J2           3500        145.179                     \n",
      "\u001b[J3           4000        133.815     127.264         \n",
      "\u001b[J3           4500        125.631                     \n",
      "\u001b[J3           5000        115.695                     \n",
      "\u001b[J4           5500        114.942     113.978         \n",
      "\u001b[J4           6000        111.734                     \n",
      "\u001b[J4           6500        100.336                     \n",
      "\u001b[J5           7000        106.212     107.006         \n",
      "\u001b[J5           7500        100.742                     \n",
      "\u001b[J6           8000        94.2726     101.633         \n",
      "\u001b[J6           8500        93.562                      \n",
      "\u001b[J6           9000        97.7327                     \n",
      "\u001b[J7           9500        81.6866     97.9659         \n",
      "\u001b[J7           10000       92.2912                     \n",
      "\u001b[J7           10500       81.803                      \n",
      "\u001b[J8           11000       82.6315     95.1748         \n",
      "\u001b[J8           11500       82.7933                     \n",
      "\u001b[J9           12000       83.4409     93.5401         \n",
      "\u001b[J9           12500       77.5522                     \n",
      "\u001b[J9           13000       75.1518                     \n",
      "\u001b[J10          13500       76.4145     92.069          \n",
      "\u001b[J10          14000       72.4509                     \n",
      "\u001b[J10          14500       69.8914                     \n",
      "\u001b[J11          15000       70.53       90.8836         \n",
      "\u001b[J11          15500       71.8515                     \n",
      "\u001b[J12          16000       66.6751     89.9575         \n",
      "\u001b[J12          16500       70.8432                     \n",
      "\u001b[J12          17000       70.1107                     \n",
      "\u001b[J13          17500       68.6903     89.5766         \n",
      "\u001b[J13          18000       67.2043                     \n",
      "\u001b[J13          18500       66.4657                     \n"
     ]
    }
   ],
   "source": [
    "updater = BPTTUpdater(train_iter, optimizer, bproplen, gpu)\n",
    "trainer = training.Trainer(updater, (epoch, 'epoch'), out='result')\n",
    "\n",
    "eval_model = model.copy()  # Model with shared params and distinct states                                                                                                                                          \n",
    "eval_rnn = eval_model.predictor\n",
    "trainer.extend(extensions.Evaluator(\n",
    "    val_iter, eval_model, device=gpu,\n",
    "    # Reset the RNN state at the beginning of each evaluation                                                                                                                                                      \n",
    "    eval_hook=lambda _: eval_rnn.reset_state()))\n",
    "\n",
    "trainer.extend(extensions.LogReport(postprocess=compute_perplexity, trigger=(interval, 'iteration')))\n",
    "trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'iteration', 'perplexity', 'val_perplexity']\n",
    "        ), trigger=(interval, 'iteration'))\n",
    "trainer.extend(extensions.snapshot())\n",
    "trainer.extend(extensions.snapshot_object(model, 'model_iter_{.updater.iteration}'))\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2.12 Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eval_rnn.reset_state()\n",
    "evaluator = extensions.Evaluator(test_iter, eval_model, device=gpu)\n",
    "result = evaluator()\n",
    "print('test perplexity:', np.exp(float(result['main/loss'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2.3 Generating sentences\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can generate the sentence which starts with a word in the vocabulary. In this example, we generate a sentence which starts with the word **apple**. We use the script in the PTB example of the official repository.\n",
    "\n",
    "https://github.com/chainer/chainer/tree/master/examples/ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python gentxt.py -m model.npz -p apple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> apple a new u.s. economist with <unk> <unk> fixed more than to N the company said who is looking back to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
