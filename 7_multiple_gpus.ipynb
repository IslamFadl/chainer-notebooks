{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a ConvNet using multiple GPUs\n",
    "\n",
    "This notebook shows how to train a deep model using multiple GPUs. In this notebook, we use CIFAR-10 dataset and train a VGG-like deep model using 2 GPUs.\n",
    "\n",
    "First, **please make sure that you are running this notebook with an environment that has at least 2 GPUs**.\n",
    "\n",
    "Let's import necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from chainer.datasets import get_cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "Chainer provides a utility to retrieve and handle the CIFAR-10/100 dataset. In this notebook, we use the dataset class provided by Chainer to use CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 128\n",
    "\n",
    "train, test = get_cifar10()\n",
    "train_iter = iterators.MultiprocessIterator(train, batchsize)\n",
    "test_iter = iterators.MultiprocessIterator(test, batchsize, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "Let's define a deep model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGG(chainer.ChainList):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super(VGG, self).__init__(\n",
    "            ConvBlock(64),\n",
    "            ConvBlock(64, True),\n",
    "            ConvBlock(128),\n",
    "            ConvBlock(128, True),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256, True),\n",
    "            LinearBlock(),\n",
    "            LinearBlock(),\n",
    "            L.Linear(None, n_class)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for f in self.children():\n",
    "            x = f(x)\n",
    "        return x\n",
    "    \n",
    "class ConvBlock(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_ch, pool_drop=False):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(ConvBlock, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv = L.Convolution2D(None, n_ch, 3, 1, 1, nobias=True, initialW=w)\n",
    "            self.bn = L.BatchNormalization(n_ch)\n",
    "        self.pool_drop = pool_drop\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn(self.conv(x)))\n",
    "        if self.pool_drop:\n",
    "            h = F.max_pooling_2d(h, 2, 2)\n",
    "            h = F.dropout(h, ratio=0.25)\n",
    "        return h\n",
    "\n",
    "class LinearBlock(chainer.Chain):\n",
    "\n",
    "    def __init__(self):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(LinearBlock, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.fc = L.Linear(None, 1024, initialW=w)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return F.dropout(F.relu(self.fc(x)), ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then instantiate the model and setup a optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = L.Classifier(VGG(n_class=10))\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ParallelUpdater\n",
    "\n",
    "To use multiple GPUs for training, use `ParallelUpdater` instead of `StarndardUpdater` for the updater given to a trainer. So what you need to do is just replacing `StandardUpdater` with `ParallelUpdater`. That's all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   val/main/loss  main/accuracy  val/main/accuracy  elapsed_time\n",
      "\u001b[J1           2.06062     1.72784        0.248561       0.345728           27.6125       \n",
      "\u001b[J2           1.51435     1.37587        0.431706       0.499407           53.2946       \n",
      "\u001b[J3           1.22403     1.08813        0.555529       0.636175           78.8932       \n",
      "\u001b[J4           1.01321     0.991406       0.639666       0.684731           106.35        \n",
      "\u001b[J5           0.901679    0.875154       0.685582       0.737144           136.299       \n",
      "\u001b[J6           0.782395    0.828507       0.732292       0.741792           165.523       \n",
      "\u001b[J7           0.710225    0.84949        0.761949       0.747033           194.874       \n",
      "\u001b[J8           0.642385    0.633132       0.790144       0.812104           224.745       \n",
      "\u001b[J9           0.574841    0.63775        0.809303       0.812698           254.329       \n",
      "\u001b[J10          0.510813    0.638685       0.831562       0.80449            283.289       \n",
      "\u001b[J11          0.472714    0.576256       0.847075       0.809929           312.512       \n",
      "\u001b[J12          0.443705    0.486639       0.857297       0.846816           341.53        \n",
      "\u001b[J13          0.392901    0.50913        0.87448        0.834454           370.354       \n",
      "\u001b[J14          0.351432    0.482847       0.8875         0.842662           398.908       \n",
      "\u001b[J15          0.33375     0.446264       0.890066       0.855815           428.561       \n",
      "\u001b[J16          0.3024      0.455186       0.901963       0.851859           457.791       \n",
      "\u001b[J17          0.266332    0.451124       0.912084       0.862441           487.5         \n",
      "\u001b[J18          0.253685    0.501251       0.915601       0.842662           516.447       \n",
      "\u001b[J19          0.2364      0.432909       0.924279       0.862638           545.575       \n",
      "\u001b[J20          0.208632    0.506819       0.933824       0.862638           575.64        \n",
      "\u001b[J21          0.194269    0.43255        0.93762        0.870451           604.736       \n",
      "\u001b[J22          0.170368    0.466          0.945393       0.866891           633.722       \n",
      "\u001b[J23          0.166821    0.529847       0.946811       0.862441           662.984       \n",
      "\u001b[J24          0.151152    0.463431       0.951082       0.874407           692.154       \n",
      "\u001b[J25          0.141251    0.459501       0.954963       0.872033           720.543       \n",
      "\u001b[J26          0.128121    0.573401       0.960038       0.858089           749.405       \n",
      "\u001b[J27          0.130896    0.456771       0.959095       0.872132           778.601       \n",
      "\u001b[J28          0.117692    0.522442       0.963035       0.870748           807.629       \n",
      "\u001b[J29          0.113074    0.490545       0.963875       0.872923           837.338       \n",
      "\u001b[J30          0.106436    0.453312       0.965705       0.88123            866.936       \n",
      "\u001b[J31          0.0936333   0.583216       0.96923        0.86966            897.011       \n",
      "\u001b[J32          0.0980956   0.519949       0.969511       0.870253           926.474       \n",
      "\u001b[J33          0.0901761   0.511225       0.971427       0.87945            955.517       \n",
      "\u001b[J34          0.087176    0.518954       0.972506       0.878263           985.729       \n",
      "\u001b[J35          0.0817941   0.573686       0.974119       0.877967           1015.35       \n",
      "\u001b[J36          0.0820182   0.511111       0.974465       0.878461           1044.64       \n",
      "\u001b[J37          0.07492     0.585388       0.977502       0.882812           1073.62       \n",
      "\u001b[J38          0.0726181   0.539151       0.977364       0.879846           1102.61       \n",
      "\u001b[J39          0.0736187   0.49998        0.977701       0.881527           1131.62       \n",
      "\u001b[J40          0.0609967   0.594223       0.98105        0.879747           1160.68       \n",
      "\u001b[J41          0.0701719   0.537505       0.978141       0.881329           1191.11       \n",
      "\u001b[J42          0.0657121   0.536152       0.980059       0.883999           1219.69       \n",
      "\u001b[J43          0.0588336   0.571397       0.98105        0.884494           1248.67       \n",
      "\u001b[J44          0.0629678   0.6295         0.981418       0.87589            1277.73       \n",
      "\u001b[J45          0.0551307   0.625942       0.982777       0.874802           1306.83       \n",
      "\u001b[J46          0.0584306   0.558424       0.98129        0.881626           1335.72       \n",
      "\u001b[J47          0.0591954   0.557666       0.982417       0.875989           1364.9        \n",
      "\u001b[J48          0.0581805   0.637992       0.982131       0.879055           1393.83       \n",
      "\u001b[J49          0.0556543   0.586167       0.984255       0.879549           1423.11       \n",
      "\u001b[J50          0.0531761   0.571888       0.983696       0.879846           1452.54       \n",
      "\u001b[J51          0.0499855   0.617992       0.984014       0.879648           1481.04       \n",
      "\u001b[J52          0.0532928   0.650794       0.983336       0.880835           1510.73       \n",
      "\u001b[J53          0.055306    0.572665       0.982976       0.882615           1539.85       \n",
      "\u001b[J54          0.0483244   0.622639       0.985857       0.88123            1570.31       \n",
      "\u001b[J55          0.0501615   0.596857       0.985294       0.881725           1600.52       \n",
      "\u001b[J56          0.0472725   0.604612       0.986258       0.882318           1630.87       \n",
      "\u001b[J57          0.0484204   0.626023       0.985214       0.879055           1661.5        \n",
      "\u001b[J58          0.0465541   0.612769       0.986213       0.884098           1690.94       \n",
      "\u001b[J59          0.0433752   0.607433       0.98758        0.877176           1720.28       \n",
      "\u001b[J60          0.0518179   0.569884       0.985534       0.880736           1750.26       \n",
      "\u001b[J61          0.0512358   0.596257       0.985334       0.884691           1779.3        \n",
      "\u001b[J62          0.0445089   0.616687       0.986338       0.879252           1807.78       \n",
      "\u001b[J63          0.0402594   0.728541       0.988171       0.875198           1836.64       \n",
      "\u001b[J64          0.0463596   0.602059       0.987139       0.883208           1865.08       \n",
      "\u001b[J65          0.0379239   0.638789       0.98929        0.879747           1894.15       \n",
      "\u001b[J66          0.0435649   0.628246       0.986973       0.879252           1923.19       \n",
      "\u001b[J67          0.0397172   0.65113        0.988662       0.879055           1951.86       \n",
      "\u001b[J68          0.0437917   0.641258       0.987132       0.880934           1981.48       \n",
      "\u001b[J69          0.0421342   0.605103       0.987652       0.884593           2010.2        \n",
      "\u001b[J70          0.0338352   0.631827       0.990385       0.888252           2038.75       \n",
      "\u001b[J71          0.0374422   0.61708        0.98949        0.881329           2066.91       \n",
      "\u001b[J72          0.0385588   0.617529       0.988141       0.885285           2095.65       \n",
      "\u001b[J73          0.0356159   0.663951       0.98957        0.885581           2126.64       \n",
      "\u001b[J74          0.0368221   0.738188       0.988771       0.879945           2155.04       \n",
      "\u001b[J75          0.0376984   0.657186       0.989784       0.876879           2183.84       \n",
      "\u001b[J76          0.0408007   0.598325       0.987932       0.879055           2212.58       \n",
      "\u001b[J77          0.0378266   0.643631       0.988331       0.880439           2241.51       \n",
      "\u001b[J78          0.0343585   0.684672       0.990024       0.881131           2270.22       \n",
      "\u001b[J79          0.0369227   0.657258       0.98929        0.886076           2299.7        \n",
      "\u001b[J80          0.0313397   0.773905       0.990946       0.876978           2329.74       \n",
      "\u001b[J81          0.0364844   0.67806        0.98941        0.887559           2359.12       \n",
      "\u001b[J82          0.0320268   0.649919       0.990529       0.882714           2388.19       \n",
      "\u001b[J83          0.0374646   0.593741       0.988902       0.886472           2417.71       \n",
      "\u001b[J84          0.0326638   0.688821       0.990569       0.875396           2447.42       \n",
      "\u001b[J85          0.0356872   0.722809       0.98965        0.87589            2477.61       \n",
      "\u001b[J86          0.0337024   0.651198       0.990064       0.882516           2507.77       \n",
      "\u001b[J87          0.0334121   0.609174       0.990249       0.885581           2537.2        \n",
      "\u001b[J88          0.0259913   0.681316       0.991947       0.879846           2565.96       \n",
      "\u001b[J89          0.0262138   0.691931       0.992048       0.883505           2595.56       \n",
      "\u001b[J90          0.0343821   0.695963       0.98969        0.887164           2624.64       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J91          0.0263309   0.635631       0.992468       0.889636           2653.49       \n",
      "\u001b[J92          0.0341086   0.616935       0.990329       0.879648           2682.73       \n",
      "\u001b[J93          0.0306678   0.775979       0.991648       0.875989           2711.84       \n",
      "\u001b[J94          0.0269641   0.671159       0.991386       0.885186           2742.81       \n"
     ]
    }
   ],
   "source": [
    "devices = {\n",
    "    'main': 0,\n",
    "    'gpu1': 1,\n",
    "}\n",
    "\n",
    "updater = training.ParallelUpdater(\n",
    "    train_iter, optimizer, devices=devices)\n",
    "\n",
    "trainer = training.Trainer(\n",
    "    updater, (100, 'epoch'), out='multi_gpu_result')\n",
    "\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'val/main/loss', 'main/accuracy', 'val/main/accuracy', 'elapsed_time']))\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=devices['main']), name='val')\n",
    "trainer.extend(extensions.PlotReport(['main/loss', 'val/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "trainer.extend(extensions.PlotReport(['main/accuracy', 'val/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('mnist_result/loss.png')\n",
    "Image('mnist_result/accuracy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChainerCV\n",
    "\n",
    "[ChainerCV](https://github.com/chainer/chainercv) is a collection of tools to train and run neural networks for computer vision tasks using Chainer. It provides many kinds of models, their pre-trained weights, data augomentation algorithms, dataset abstraction, evaluation tools, etc.\n",
    "\n",
    "In this notebook, let's use ChainerCV to apply some transformations to the training images for data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install chainercv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainer's `TransformDataset` takes two arguments, the first one is a dataset object and the second one is a transformation function. Each datum in the dataset is transformed by using the transformation function. The function should take an input and perform some transformations using any kind of libraries or just array maniputation to the input ndarray, and then return the resulting transformed input.\n",
    "\n",
    "Let's train the same model with the `TransformDataset` that apply\n",
    "\n",
    "- Random cropping of (28, 28)-sized sub-region\n",
    "- Random LR-flipping\n",
    "- Random color augmentation\n",
    "\n",
    "using `chainercv.transforms.random_crop`, `chainercv.transforms.random_flip`, and `chainercv.transforms.pca_lighting`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from chainercv import transforms\n",
    "\n",
    "def transform(inputs):\n",
    "    img, label = inputs\n",
    "    img = transforms.random_crop(img, (28, 28))\n",
    "    img = transforms.random_flip(img)\n",
    "    img = transforms.pca_lighting(img, 0.1)\n",
    "    return img, label\n",
    "    \n",
    "batchsize = 128\n",
    "\n",
    "train, test = get_cifar10()\n",
    "\n",
    "train = datasets.TransformDataset(train, transform)\n",
    "\n",
    "train_iter = iterators.MultiprocessIterator(train, batchsize)\n",
    "test_iter = iterators.SerialIterator(test, batchsize, repeat=False, shuffle=False)\n",
    "\n",
    "model = L.Classifier(VGG(n_class=10))\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "devices = {\n",
    "    'main': 0,\n",
    "    'gpu1': 1,\n",
    "}\n",
    "updater = training.ParallelUpdater(\n",
    "    train_iter, optimizer, devices=devices)\n",
    "\n",
    "trainer = training.Trainer(\n",
    "    updater, (100, 'epoch'), out='multi_gpu_result_2')\n",
    "\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'val/main/loss', 'main/accuracy', 'val/main/accuracy', 'elapsed_time']))\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=devices['main']), name='val')\n",
    "trainer.extend(extensions.PlotReport(['main/loss', 'val/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "trainer.extend(extensions.PlotReport(['main/accuracy', 'val/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
