{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a ConvNet using multiple GPUs\n",
    "\n",
    "This notebook shows how to train a deep model using multiple GPUs. In this notebook, we use CIFAR-10 dataset and train ResNet using 2 GPUs.\n",
    "\n",
    "First, **please make sure that you are running this notebook with an environment that has at least 2 GPUs**.\n",
    "\n",
    "Let's import necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from chainer.datasets import get_cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "Chainer provides a utility to retrieve and handle the CIFAR-10/100 dataset. In this notebook, we use the dataset class provided by Chainer to use CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz...\n"
     ]
    }
   ],
   "source": [
    "batchsize = 128\n",
    "\n",
    "train, test = get_cifar10()\n",
    "train_iter = iterators.MultiprocessIterator(train, batchsize)\n",
    "test_iter = iterators.SerialIterator(test, batchsize, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "Let's define a deep model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNet50(chainer.Chain):\n",
    "    def __init__(self, n_class, n_blocks=[3, 4, 6, 3]):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(ResNet50, self).__init__(\n",
    "            conv1=L.Convolution2D(\n",
    "                None, 64, 7, 2, 3, initialW=w, nobias=True),\n",
    "            bn1=L.BatchNormalization(64),\n",
    "            res2=ResBlock(n_blocks[0], 64, 64, 256, 1),\n",
    "            res3=ResBlock(n_blocks[1], 256, 128, 512),\n",
    "            res4=ResBlock(n_blocks[2], 512, 256, 1024),\n",
    "            res5=ResBlock(n_blocks[3], 1024, 512, 2048),\n",
    "            fc6=L.Linear(2048, n_class))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self.bn1(self.conv1(x))\n",
    "        h = F.max_pooling_2d(F.relu(h), 2, 2)\n",
    "        h = self.res2(h)\n",
    "        h = self.res3(h)\n",
    "        h = self.res4(h)\n",
    "        h = self.res5(h)\n",
    "        h = F.average_pooling_2d(h, h.shape[2:], stride=1)\n",
    "        h = self.fc6(h)\n",
    "        if chainer.config.train:\n",
    "            return h\n",
    "        return F.softmax(h)\n",
    "\n",
    "\n",
    "class ResBlock(chainer.ChainList):\n",
    "    def __init__(self, n_layers, n_in, n_mid, n_out, stride=2):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.add_link(BottleNeck(n_in, n_mid, n_out, stride, True))\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.add_link(BottleNeck(n_out, n_mid, n_out))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for f in self.children():\n",
    "            x = f(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleNeck(chainer.Chain):\n",
    "    def __init__(self, n_in, n_mid, n_out, stride=1, proj=False):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(BottleNeck, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1x1a = L.Convolution2D(\n",
    "                n_in, n_mid, 1, stride, 0, initialW=w, nobias=True)\n",
    "            self.conv3x3b = L.Convolution2D(\n",
    "                n_mid, n_mid, 3, 1, 1, initialW=w, nobias=True)\n",
    "            self.conv1x1c = L.Convolution2D(\n",
    "                n_mid, n_out, 1, 1, 0, initialW=w, nobias=True)\n",
    "            self.bn_a = L.BatchNormalization(n_mid)\n",
    "            self.bn_b = L.BatchNormalization(n_mid)\n",
    "            self.bn_c = L.BatchNormalization(n_out)\n",
    "            if proj:\n",
    "                self.conv1x1r = L.Convolution2D(\n",
    "                    n_in, n_out, 1, stride, 0, initialW=w, nobias=True)\n",
    "                self.bn_r = L.BatchNormalization(n_out)\n",
    "        self.proj = proj\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn_a(self.conv1x1a(x)))\n",
    "        h = F.relu(self.bn_b(self.conv3x3b(h)))\n",
    "        h = self.bn_c(self.conv1x1c(h))\n",
    "        if self.proj:\n",
    "            x = self.bn_r(self.conv1x1r(x))\n",
    "        return F.relu(h + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then instantiate the model and setup a optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = L.Classifier(ResNet50(n_class=10))\n",
    "optimizer = optimizers.MomentumSGD(lr=0.01)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ParallelUpdater\n",
    "\n",
    "To use multiple GPUs for training, use `ParallelUpdater` instead of `StarndardUpdater` for the updater given to a trainer. So what you need to do is just replacing `StandardUpdater` with `ParallelUpdater`. That's all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = {\n",
    "    'main': 1,\n",
    "    'gpu1': 2\n",
    "}\n",
    "\n",
    "updater = training.ParallelUpdater(\n",
    "    train_iter, optimizer, devices=devices)\n",
    "\n",
    "trainer = training.Trainer(\n",
    "    updater, (100, 'epoch'), out='multi_gpu_result')\n",
    "\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'val/main/loss', 'main/accuracy', 'val/main/accuracy', 'elapsed_time']))\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=devices['main']), name='val')\n",
    "trainer.extend(extensions.ExponentialShift('lr', 0.5), trigger=(25, 'epoch'))\n",
    "trainer.extend(extensions.PlotReport(['main/loss', 'val/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "trainer.extend(extensions.PlotReport(['main/accuracy', 'val/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('mnist_result/loss.png')\n",
    "Image('mnist_result/accuracy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChainerCV\n",
    "\n",
    "[ChainerCV](https://github.com/chainer/chainercv) is a collection of tools to train and run neural networks for computer vision tasks using Chainer. It provides many kinds of models, their pre-trained weights, data augomentation algorithms, dataset abstraction, evaluation tools, etc.\n",
    "\n",
    "In this notebook, let's use ChainerCV to apply some transformations to the training images for data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install chainercv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainer's `TransformDataset` takes two arguments, the first one is a dataset object and the second one is a transformation function. Each datum in the dataset is transformed by using the transformation function. The function should take an input and perform some transformations using any kind of libraries or just array maniputation to the input ndarray, and then return the resulting transformed input.\n",
    "\n",
    "Let's train the same model with the `TransformDataset` that apply random LR-flipping and random color augmentation using `chainercv.transforms.random_flip` and `chainercv.transforms.pca_lighting`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from chainercv import transforms\n",
    "\n",
    "def transform(inputs):\n",
    "    img, label = inputs\n",
    "    img = transforms.random_flip(img)\n",
    "    img = transforms.pca_lighting(img, 0.1)\n",
    "    return img, label\n",
    "    \n",
    "batchsize = 128\n",
    "\n",
    "train, test = get_cifar10()\n",
    "\n",
    "train = datasets.TransformDataset(train, transform)\n",
    "\n",
    "train_iter = iterators.MultiprocessIterator(train, batchsize)\n",
    "test_iter = iterators.SerialIterator(test, batchsize, repeat=False, shuffle=False)\n",
    "\n",
    "model = L.Classifier(ResNet50(n_class=10))\n",
    "optimizer = optimizers.MomentumSGD(lr=0.01)\n",
    "optimizer.setup(model)\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(5e-4))\n",
    "\n",
    "devices = {\n",
    "    'main': 1,\n",
    "    'gpu1': 2\n",
    "}\n",
    "\n",
    "updater = training.ParallelUpdater(\n",
    "    train_iter, optimizer, devices=devices)\n",
    "\n",
    "trainer = training.Trainer(\n",
    "    updater, (100, 'epoch'), out='multi_gpu_result')\n",
    "\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'val/main/loss', 'main/accuracy', 'val/main/accuracy', 'elapsed_time']))\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=devices['main']), name='val')\n",
    "trainer.extend(extensions.ExponentialShift('lr', 0.5), trigger=(25, 'epoch'))\n",
    "trainer.extend(extensions.PlotReport(['main/loss', 'val/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "trainer.extend(extensions.PlotReport(['main/accuracy', 'val/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
