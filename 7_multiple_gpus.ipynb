{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a ConvNet using multiple GPUs\n",
    "\n",
    "This notebook shows how to train a deep model using multiple GPUs. In this notebook, we use CIFAR-10 dataset and train a VGG-like deep model using 2 GPUs.\n",
    "\n",
    "First, **please make sure that you are running this notebook with an environment that has at least 2 GPUs**.\n",
    "\n",
    "Let's import necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from chainer.datasets import get_cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "Chainer provides a utility to retrieve and handle the CIFAR-10/100 dataset. In this notebook, we use the dataset class provided by Chainer to use CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchsize = 128\n",
    "\n",
    "train, test = get_cifar10()\n",
    "train_iter = iterators.MultiprocessIterator(train, batchsize)\n",
    "test_iter = iterators.MultiprocessIterator(test, batchsize, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "Let's define a deep model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGG(chainer.ChainList):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super(VGG, self).__init__(\n",
    "            ConvBlock(64),\n",
    "            ConvBlock(64, True),\n",
    "            ConvBlock(128),\n",
    "            ConvBlock(128, True),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256),\n",
    "            ConvBlock(256, True),\n",
    "            LinearBlock(),\n",
    "            LinearBlock(),\n",
    "            L.Linear(None, n_class)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for f in self.children():\n",
    "            x = f(x)\n",
    "        return x\n",
    "    \n",
    "class ConvBlock(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_ch, pool_drop=False):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(ConvBlock, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv = L.Convolution2D(None, n_ch, 3, 1, 1, nobias=True, initialW=w)\n",
    "            self.bn = L.BatchNormalization(n_ch)\n",
    "        self.pool_drop = pool_drop\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.bn(self.conv(x)))\n",
    "        if self.pool_drop:\n",
    "            h = F.max_pooling_2d(h, 2, 2)\n",
    "            h = F.dropout(h, ratio=0.25)\n",
    "        return h\n",
    "\n",
    "class LinearBlock(chainer.Chain):\n",
    "\n",
    "    def __init__(self):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(LinearBlock, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.fc = L.Linear(None, 1024, initialW=w)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return F.dropout(F.relu(self.fc(x)), ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then instantiate the model and setup a optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = L.Classifier(VGG(n_class=10))\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ParallelUpdater\n",
    "\n",
    "To use multiple GPUs for training, use `ParallelUpdater` instead of `StarndardUpdater` for the updater given to a trainer. So what you need to do is just replacing `StandardUpdater` with `ParallelUpdater`. That's all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   val/main/loss  main/accuracy  val/main/accuracy  elapsed_time\n",
      "\u001b[J1           2.03247     1.59413        0.259471       0.388153           31.5463       \n",
      "\u001b[J2           1.48999     1.35446        0.44805        0.526108           61.2136       \n",
      "\u001b[J3           1.2023      1.13322        0.570152       0.624407           91.5109       \n",
      "\u001b[J4           0.990455    1.18442        0.653932       0.60265            121.103       \n",
      "\u001b[J5           0.866919    0.895297       0.701686       0.727354           152.11        \n",
      "\u001b[J6           0.761392    0.828588       0.740625       0.744858           182.07        \n",
      "\u001b[J7           0.688458    0.738975       0.77062        0.783129           212.247       \n",
      "\u001b[J8           0.610161    0.706257       0.798678       0.785898           242.723       \n"
     ]
    }
   ],
   "source": [
    "devices = {\n",
    "    'main': 0,\n",
    "    'gpu1': 1,\n",
    "}\n",
    "\n",
    "updater = training.ParallelUpdater(\n",
    "    train_iter, optimizer, devices=devices)\n",
    "\n",
    "trainer = training.Trainer(\n",
    "    updater, (100, 'epoch'), out='multi_gpu_result')\n",
    "\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'val/main/loss', 'main/accuracy', 'val/main/accuracy', 'elapsed_time']))\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=devices['main']), name='val')\n",
    "trainer.extend(extensions.PlotReport(['main/loss', 'val/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "trainer.extend(extensions.PlotReport(['main/accuracy', 'val/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('mnist_result/loss.png')\n",
    "Image('mnist_result/accuracy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChainerCV\n",
    "\n",
    "[ChainerCV](https://github.com/chainer/chainercv) is a collection of tools to train and run neural networks for computer vision tasks using Chainer. It provides many kinds of models, their pre-trained weights, data augomentation algorithms, dataset abstraction, evaluation tools, etc.\n",
    "\n",
    "In this notebook, let's use ChainerCV to apply some transformations to the training images for data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install chainercv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainer's `TransformDataset` takes two arguments, the first one is a dataset object and the second one is a transformation function. Each datum in the dataset is transformed by using the transformation function. The function should take an input and perform some transformations using any kind of libraries or just array maniputation to the input ndarray, and then return the resulting transformed input.\n",
    "\n",
    "Let's train the same model with the `TransformDataset` that apply random LR-flipping and random color augmentation using `chainercv.transforms.random_flip` and `chainercv.transforms.pca_lighting`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from chainercv import transforms\n",
    "\n",
    "def transform(inputs):\n",
    "    img, label = inputs\n",
    "    img = transforms.random_flip(img)\n",
    "    img = transforms.pca_lighting(img, 0.1)\n",
    "    return img, label\n",
    "    \n",
    "batchsize = 128\n",
    "\n",
    "train, test = get_cifar10()\n",
    "\n",
    "train = datasets.TransformDataset(train, transform)\n",
    "\n",
    "train_iter = iterators.MultiprocessIterator(train, batchsize)\n",
    "test_iter = iterators.SerialIterator(test, batchsize, repeat=False, shuffle=False)\n",
    "\n",
    "model = L.Classifier(VGG(n_class=10))\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "devices = {\n",
    "    'main': 0,\n",
    "    'gpu1': 1,\n",
    "}\n",
    "updater = training.ParallelUpdater(\n",
    "    train_iter, optimizer, devices=devices)\n",
    "\n",
    "trainer = training.Trainer(\n",
    "    updater, (100, 'epoch'), out='multi_gpu_result')\n",
    "\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'val/main/loss', 'main/accuracy', 'val/main/accuracy', 'elapsed_time']))\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=devices['main']), name='val')\n",
    "trainer.extend(extensions.PlotReport(['main/loss', 'val/main/loss'], x_key='epoch', file_name='loss.png'))\n",
    "trainer.extend(extensions.PlotReport(['main/accuracy', 'val/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n",
    "\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
