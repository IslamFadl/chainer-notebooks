{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla LSTM with CuPy\n",
    "\n",
    "This is a port of [Vanilla LSTM with numpy](http://blog.varunajayasiri.com/numpy_lstm.html) that shows how to run the numpy-based machine learning code on GPU using CuPy. The all contents below is basically copied from the article: [Vanilla LSTM with numpy](http://blog.varunajayasiri.com/numpy_lstm.html).\n",
    "\n",
    "This is inspired from [Minimal character-level language model with a Vanilla Recurrent Neural Network](https://gist.github.com/karpathy/d4dee566867f8291f086), in Python/numpy by [Andrej Karpathy](https://github.com/karpathy).\n",
    "\n",
    "The model usually reaches an error of about 45 after 5000 iterations when tested with [100,000 character sample from Shakespeare](http://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt). However it sometimes get stuck in a local minima; reinitialize the weights if this happens.\n",
    "\n",
    "You need to place the input text file as `input.txt` in the same folder as the python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      " 15 99993   15 15670    0     0  13155      0  0:00:07  0:00:01  0:00:06 42123\r",
      "100 99993  100 99993    0     0  69435      0  0:00:01  0:00:01 --:--:--  157k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -L http://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt -o input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above imports are from the original article as is. But in this article, you can run almost all computation on GPU by just replacing `np` with `cp`. Well, `cp` is just another name of `cupy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the difference of computational time by switching CPU and GPU simply, let's use `xp` instead of `np` and switch the referenced package between `numpy` and `cupy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you use CPU\n",
    "# xp = np\n",
    "\n",
    "# If you use GPU\n",
    "xp = cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Because CuPy has high compatibility with NumPy, the same code using NumPy is super easily converted for CuPy by just replacing `numpy` with `cupy`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('input.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data and calculate indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 99993 characters, 62 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions and Derivatives\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) &=& \\frac{1}{1 + e^{-x}} \\\\\n",
    "\\frac{d \\sigma(x)}{d x} &=& \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "### Tanh\n",
    "\n",
    "$$\n",
    "\\frac{d \\tanh(x)}{dx} = 1 - \\tanh^2(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + xp.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return xp.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights\n",
    "\n",
    "We use random weights with normal distribution `(0, weight_sd)` for `tanh` activation function and `(0.5, weight_sd)` for `sigmoid` activation function.\n",
    "\n",
    "Biases are initialized to zeros.\n",
    "\n",
    "Formulae for LSTM are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_f = xp.random.randn(H_size, z_size) * weight_sd + 0.5\n",
    "b_f = xp.zeros((H_size, 1))\n",
    "\n",
    "W_i = xp.random.randn(H_size, z_size) * weight_sd + 0.5\n",
    "b_i = xp.zeros((H_size, 1))\n",
    "\n",
    "W_C = xp.random.randn(H_size, z_size) * weight_sd\n",
    "b_C = xp.zeros((H_size, 1))\n",
    "\n",
    "W_o = xp.random.randn(H_size, z_size) * weight_sd + 0.5\n",
    "b_o = xp.zeros((H_size, 1))\n",
    "\n",
    "#For final layer to predict the next character\n",
    "W_y = xp.random.randn(X_size, H_size) * weight_sd\n",
    "b_y = xp.zeros((X_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dW_f = xp.zeros_like(W_f)\n",
    "dW_i = xp.zeros_like(W_i)\n",
    "dW_C = xp.zeros_like(W_C)\n",
    "\n",
    "dW_o = xp.zeros_like(W_o)\n",
    "dW_y = xp.zeros_like(W_y)\n",
    "\n",
    "db_f = xp.zeros_like(b_f)\n",
    "db_i = xp.zeros_like(b_i)\n",
    "db_C = xp.zeros_like(b_C)\n",
    "\n",
    "db_o = xp.zeros_like(b_o)\n",
    "db_y = xp.zeros_like(b_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "\n",
    "Image taken from [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). Please read the article for a good explanation of LSTMs.\n",
    "\n",
    "### Concatenation of $h_{t-1}$ and $x_t$\n",
    "\n",
    "$$\n",
    "z = [h_{t-1}, x_t]\n",
    "$$\n",
    "\n",
    "### LSTM functions\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "f_t &=& \\sigma(W_f \\cdot z + b_f) \\\\\n",
    "i_t &=& \\sigma(W_i \\cdot z + b_i) \\\\\n",
    "\\bar{C}_t &=& \\tanh(W_C \\cdot z + b_C) \\\\\n",
    "C_t &=& f_t \\ast C_{t-1} + i_t \\ast \\bar{C}_t \\\\\n",
    "o_t &=& \\sigma(W_O \\cdot z + b_t) \\\\\n",
    "h_t &=& o_t \\ast \\tanh(C_t)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "### Logits\n",
    "\n",
    "$$\n",
    "y_t = W_y \\cdot h_t + b_y\n",
    "$$\n",
    "\n",
    "### Softmax\n",
    "\n",
    "$$\n",
    "\\hat{p}_t = {\\rm softmax}(y_t)\n",
    "$$\n",
    "\n",
    "$\\hat{p}_t$ is `p` in code and $p_t$ is `targets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)\n",
    "    assert C_prev.shape == (H_size, 1)\n",
    "\n",
    "    z = xp.concatenate((h_prev, x))\n",
    "    f = sigmoid(xp.dot(W_f, z) + b_f)\n",
    "    i = sigmoid(xp.dot(W_i, z) + b_i)\n",
    "    C_bar = tanh(xp.dot(W_C, z) + b_C)\n",
    "\n",
    "    C = f * C_prev + i * C_bar\n",
    "    o = sigmoid(xp.dot(W_o, z) + b_o)\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    y = xp.dot(W_y, h) + b_y\n",
    "    p = xp.exp(y) / xp.sum(xp.exp(y))\n",
    "\n",
    "    return z, f, i, C_bar, C, o, h, y, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass\n",
    "\n",
    "### Loss\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\sum p_{t,j}log \\hat{p}_{t,j}\n",
    "$$\n",
    "\n",
    "### Gradients\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "dy_t &=& \\hat{p}_t - p_t \\\\\n",
    "dh_t &=& dh'_{t+1} + W_y^T \\cdot d_y \\\\\n",
    "do_t &=& dh_t \\ast \\tanh (C_t) \\\\\n",
    "dC_t &=& dC'_{t+q} + dh_t \\ast o_t \\ast (1 - \\tanh^2(C_t)) \\\\\n",
    "d \\bar{C}_t &=& d C_t \\ast i_t \\\\\n",
    "d i_t &=& d C_t \\ast \\bar{C}_t \\\\\n",
    "d f_t &=& d C_t \\ast C_{t-1} \\\\\n",
    "d z_t &=& W_f^T \\cdot df_t + W_i^T \\cdot di_t + W_C^T \\cdot \\bar{C}_t + W^T_o \\cdot do_t \\\\\n",
    "[dh'_t, dx_t] &=& dz_t \\\\\n",
    "dC'_t &=& f \\ast dC_t\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "- `target` is target character index $p_t$\n",
    "- `dh_next` is $dh_{t+1}$ (size $H \\times 1$)\n",
    "- `dC_next` is $dC_{t+1}$ (size $H \\times 1$)\n",
    "- `C_prev` is $C_{t-1}$ (size $H \\times 1$)\n",
    "- Returns $dh_t$ and $dC_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev, z, f, i, C_bar, C, o, h, y, p):\n",
    "\n",
    "    global dW_f, dW_i, dW_C, dW_o, dW_y\n",
    "    global db_f, db_i, db_C, db_o, db_y\n",
    "\n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    assert p.shape == (X_size, 1)\n",
    "\n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "\n",
    "    dy = xp.copy(p)\n",
    "    dy[target] -= 1\n",
    "\n",
    "    dW_y += xp.dot(dy, h.T)\n",
    "    db_y += dy\n",
    "\n",
    "    dh = xp.dot(W_y.T, dy)\n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    dW_o += xp.dot(do, z.T)\n",
    "    db_o += do\n",
    "\n",
    "    dC = xp.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dC_bar * dtanh(C_bar)\n",
    "    dW_C += xp.dot(dC_bar, z.T)\n",
    "    db_C += dC_bar\n",
    "\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    dW_i += xp.dot(di, z.T)\n",
    "    db_i += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    dW_f += xp.dot(df, z.T)\n",
    "    db_f += df\n",
    "\n",
    "    dz = xp.dot(W_f.T, df) \\\n",
    "        + xp.dot(W_i.T, di) \\\n",
    "        + xp.dot(W_C.T, dC_bar) \\\n",
    "        + xp.dot(W_o.T, do)\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "\n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Backward Pass\n",
    "\n",
    "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
    "\n",
    "- `input`, `target` are list of integers, with character indexes.\n",
    "- `h_prev` is the array of initial `h` at $h_1$ (size $H \\times 1$)\n",
    "- `C_prev` is the array of initial `C` at $C_1$ (size $H \\times 1$)\n",
    "- Returns loss, final $h_T$ and $C_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, f_s, i_s, C_bar_s, C_s, o_s, h_s, y_s, p_s = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    # Values at t - 1\n",
    "    h_s[-1] = xp.copy(h_prev)\n",
    "    C_s[-1] = xp.copy(C_prev)\n",
    "\n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = xp.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "\n",
    "        z_s[t], f_s[t], i_s[t], C_bar_s[t], C_s[t], o_s[t], h_s[t], y_s[t], p_s[t] \\\n",
    "            = forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "\n",
    "        loss += -xp.log(p_s[t][targets[t], 0]) # Loss for at t\n",
    "\n",
    "\n",
    "    for dparam in [dW_f, dW_i, dW_C, dW_o, dW_y, db_f, db_i, db_C, db_o, db_y]:\n",
    "        dparam.fill(0)\n",
    "\n",
    "    dh_next = xp.zeros_like(h_s[0]) #dh from the next character\n",
    "    dC_next = xp.zeros_like(C_s[0]) #dh from the next character\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        dh_next, dC_next = backward(target = targets[t], dh_next = dh_next, dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                 z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t], C = C_s[t], o = o_s[t],\n",
    "                 h = h_s[t], y = y_s[t], p = p_s[t])\n",
    "\n",
    "    # Clip gradients to mitigate exploding gradients\n",
    "    for dparam in [dW_f, dW_i, dW_C, dW_o, dW_y, db_f, db_i, db_C, db_o, db_y]:\n",
    "        xp.clip(dparam, -1, 1, out=dparam)\n",
    "\n",
    "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
    "    x = xp.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    h = h_prev\n",
    "    C = C_prev\n",
    "\n",
    "    indexes = []\n",
    "    for t in range(sentence_length):\n",
    "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
    "        idx = xp.random.choice(xp.arange(X_size), size=(1,), p=p.ravel())\n",
    "        x = xp.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Adagrad)\n",
    "\n",
    "$$\n",
    "w = w - \\eta \\frac{dw}{\\sum dw^2_{\\tau}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, C_prev):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "\n",
    "    # Get predictions for 200 letters with current model\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.clf()\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory variables for Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mW_f = xp.zeros_like(W_f)\n",
    "mW_i = xp.zeros_like(W_i)\n",
    "mW_C = xp.zeros_like(W_C)\n",
    "mW_o = xp.zeros_like(W_o)\n",
    "mW_y = xp.zeros_like(W_y)\n",
    "\n",
    "mb_f = xp.zeros_like(b_f)\n",
    "mb_i = xp.zeros_like(b_i)\n",
    "mb_C = xp.zeros_like(b_C)\n",
    "mb_o = xp.zeros_like(b_o)\n",
    "mb_y = xp.zeros_like(b_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -xp.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, p = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = np.zeros((0))\n",
    "plot_loss = np.zeros((0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nckz0REQux0RPRORy/w91t4NCft1hPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f565b8f3860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " entust, amaith of ouch le sept would\n",
      "Whas 'dist itsbe wither, my maor; dapt ofer, stay thes you live juse hith than'ge pranged this moge spowire I dos. as of siin he grort fare foreco; the ut pame the \n",
      "----\n",
      "iter 3500, loss 51.444035\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while True:\n",
    "    # Try catch for interruption\n",
    "    try:\n",
    "        # Reset\n",
    "        if p + T_steps >= len(data) or iteration == 0:\n",
    "            g_h_prev = xp.zeros((H_size, 1))\n",
    "            g_C_prev = xp.zeros((H_size, 1))\n",
    "            p = 0\n",
    "\n",
    "\n",
    "        inputs = [char_to_idx[ch] for ch in data[p: p + T_steps]]\n",
    "        targets = [char_to_idx[ch] for ch in data[p + 1: p + T_steps + 1]]\n",
    "\n",
    "        loss, g_h_prev, g_C_prev =  forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "        # Print every hundred steps\n",
    "        if iteration % 100 == 0:\n",
    "            update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "\n",
    "        # Update weights\n",
    "        for param, dparam, mem in zip([W_f, W_i, W_C, W_o, W_y, b_f, b_i, b_C, b_o, b_y],\n",
    "                                      [dW_f, dW_i, dW_C, dW_o, dW_y, db_f, db_i, db_C, db_o, db_y],\n",
    "                                      [mW_f, mW_i, mW_C, mW_o, mW_y, mb_f, mb_i, mb_C, mb_o, mb_y]):\n",
    "            mem += dparam * dparam # Calculate sum of gradients\n",
    "            #print(learning_rate * dparam)\n",
    "            param += -(learning_rate * dparam / xp.sqrt(mem + 1e-8))\n",
    "\n",
    "        plot_iter = np.append(plot_iter, [iteration])\n",
    "        if isinstance(loss, cp.ndarray):\n",
    "            loss = loss.get()\n",
    "        plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "        p += T_steps\n",
    "        iteration += 1\n",
    "        \n",
    "        if iteration >= 5000:\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        update_status(inputs, g_h_prev, g_C_prev)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Check\n",
    "\n",
    "Approximate the numerical gradients by changing parameters and running the model. Check if the approximated gradients are equal to the computed analytical gradients (by backpropagation).\n",
    "\n",
    "Try this on `num_checks` individual paramters picked randomly for each weight matrix and bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "\n",
    "\n",
    "def gradient_check(inputs, target, h_prev, C_prev):\n",
    "    global W_f, W_i, W_C, W_o, W_y, b_f, b_i, b_C, b_o, b_y\n",
    "    global dW_f, dW_i, dW_C, dW_o, dW_y, db_f, db_i, db_C, db_o, db_y\n",
    "\n",
    "    num_checks = 10 # Number of parameters to test\n",
    "    delta = 1e-5 # The change to make on the parameter\n",
    "\n",
    "    # To calculate computed gradients\n",
    "    _, _, _ =  forward_backward(inputs, targets, h_prev, C_prev)\n",
    "\n",
    "    for param, dparam, name in zip([W_f, W_i, W_C, W_o, W_y, b_f, b_i, b_C, b_o, b_y],\n",
    "                                 [dW_f, dW_i, dW_C, dW_o, dW_y, db_f, db_i, db_C, db_o, db_y],\n",
    "                                 ['W_f', 'W_i', 'W_C', 'W_o', 'W_y', 'b_f', 'b_i', 'b_C', 'b_o', 'b_y']):\n",
    "        assert param.shape == dparam.shape\n",
    "        dparam_copy = xp.copy(dparam) #Make a copy because this will get modified\n",
    "\n",
    "        # Test num_checks times\n",
    "        for i in range(num_checks):\n",
    "            # Pick a random index\n",
    "            rnd_idx = int(uniform(0,param.size))\n",
    "\n",
    "            # evaluate cost at [x + delta] and [x - delta]\n",
    "            old_val = xp.ravel(param)[rnd_idx]\n",
    "            xp.ravel(param)[rnd_idx] = old_val + delta\n",
    "            loss_plus_delta, _, _ = forward_backward(inputs, targets, h_prev, C_prev)\n",
    "            xp.ravel(param)[rnd_idx] = old_val - delta\n",
    "            loss_mins_delta, _, _ = forward_backward(inputs, targets, h_prev, C_prev)\n",
    "            xp.ravel(param)[rnd_idx] = old_val\n",
    "\n",
    "            grad_analytical = xp.ravel(dparam_copy)[rnd_idx]\n",
    "            grad_numerical = (loss_plus_delta - loss_mins_delta) / (2 * delta)\n",
    "            # Clip numerical error because grad_analytical is clipped\n",
    "            grad_numerical = xp.clip(grad_numerical, -1, 1)\n",
    "\n",
    "\n",
    "            err_sum = abs(grad_numerical + grad_analytical) + 1e-09\n",
    "            rel_error = abs(grad_analytical - grad_numerical) / err_sum\n",
    "\n",
    "            # If relative error is greater than 1e-06\n",
    "            if rel_error > 1e-06:\n",
    "                print('%s (%e, %e) => %e' % (name, grad_numerical, grad_analytical, rel_error))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_f (9.759837e-05, 1.951978e-04) => 3.333345e-01\n",
      "W_f (2.005006e-04, 4.010012e-04) => 3.333328e-01\n",
      "W_f (3.479740e-03, 6.959446e-03) => 3.333312e-01\n",
      "W_f (-4.219345e-05, -8.438696e-05) => 3.333310e-01\n",
      "W_f (-2.533693e-03, -5.067387e-03) => 3.333333e-01\n",
      "W_f (2.396477e-03, 4.792965e-03) => 3.333343e-01\n",
      "W_i (-7.388724e-04, -1.477747e-03) => 3.333338e-01\n",
      "W_i (2.034393e-02, 4.068785e-02) => 3.333333e-01\n",
      "W_i (9.784582e-03, 1.956921e-02) => 3.333345e-01\n",
      "W_i (5.395754e-03, 1.079151e-02) => 3.333335e-01\n",
      "W_i (-1.508203e-02, -3.016406e-02) => 3.333335e-01\n",
      "W_C (2.333694e-03, 4.667353e-03) => 3.333300e-01\n",
      "W_C (-4.268897e-02, -8.537837e-02) => 3.333356e-01\n",
      "W_C (3.288577e-04, 6.577154e-04) => 3.333330e-01\n",
      "W_C (5.022027e-04, 1.004405e-03) => 3.333330e-01\n",
      "W_C (-6.847500e-06, -1.369581e-05) => 3.333433e-01\n",
      "W_C (-6.811366e-04, -1.362273e-03) => 3.333331e-01\n",
      "W_C (-9.348401e-04, -1.869676e-03) => 3.333322e-01\n",
      "W_o (4.380212e-03, 8.760422e-03) => 3.333332e-01\n",
      "W_o (-8.482059e-03, -1.696411e-02) => 3.333331e-01\n",
      "W_o (2.906756e-04, 5.813510e-04) => 3.333329e-01\n",
      "W_o (8.859029e-02, 1.771804e-01) => 3.333328e-01\n",
      "W_o (-4.234018e-05, -8.468010e-05) => 3.333294e-01\n",
      "W_o (9.413632e-03, 1.882726e-02) => 3.333333e-01\n",
      "W_y (1.575799e-03, 3.151598e-03) => 3.333331e-01\n",
      "W_y (5.734401e-03, 1.146878e-02) => 3.333327e-01\n",
      "W_y (3.649763e-04, 7.299520e-04) => 3.333327e-01\n",
      "W_y (1.200817e-06, 2.400980e-06) => 3.331196e-01\n",
      "W_y (-1.367967e-03, -2.735936e-03) => 3.333335e-01\n",
      "W_y (-6.195933e-06, -1.240192e-05) => 3.336757e-01\n",
      "W_y (-9.728648e-04, -1.945739e-03) => 3.333352e-01\n",
      "W_y (-8.777690e-06, -1.755603e-05) => 3.333373e-01\n",
      "W_y (2.838950e-03, 5.677892e-03) => 3.333327e-01\n",
      "W_y (1.921947e-05, 3.843776e-05) => 3.333139e-01\n",
      "b_f (1.596521e-03, 3.193024e-03) => 3.333307e-01\n",
      "b_f (-1.920477e-02, -3.840955e-02) => 3.333336e-01\n",
      "b_f (5.581873e-03, 1.116374e-02) => 3.333332e-01\n",
      "b_f (-9.305812e-03, -1.861155e-02) => 3.333317e-01\n",
      "b_f (-1.108127e-05, -2.216469e-05) => 3.333664e-01\n",
      "b_f (-1.129833e-02, -2.259661e-02) => 3.333324e-01\n",
      "b_f (-1.529662e-01, -3.059368e-01) => 3.333397e-01\n",
      "b_f (2.141263e-04, 4.282652e-04) => 3.333459e-01\n",
      "b_f (7.437233e-04, 1.487407e-03) => 3.333213e-01\n",
      "b_f (5.581873e-03, 1.116374e-02) => 3.333332e-01\n",
      "b_i (-2.877966e-02, -5.755936e-02) => 3.333336e-01\n",
      "b_i (3.532077e-02, 7.064162e-02) => 3.333339e-01\n",
      "b_i (3.715057e-02, 7.430120e-02) => 3.333336e-01\n",
      "b_i (-4.479166e-02, -8.958341e-02) => 3.333337e-01\n",
      "b_i (1.255462e-02, 2.510934e-02) => 3.333352e-01\n",
      "b_i (-3.397788e-02, -6.795593e-02) => 3.333345e-01\n",
      "b_i (4.467115e-02, 8.934203e-02) => 3.333320e-01\n",
      "b_i (-2.341305e-03, -4.682591e-03) => 3.333314e-01\n",
      "b_i (-1.014812e-03, -2.029618e-03) => 3.333321e-01\n",
      "b_i (2.458895e-02, 4.917792e-02) => 3.333335e-01\n",
      "b_C (6.023895e-02, 1.204769e-01) => 3.333295e-01\n",
      "b_C (1.542159e-01, 3.084274e-01) => 3.333270e-01\n",
      "b_C (-1.199190e-01, -2.398425e-01) => 3.333417e-01\n",
      "b_C (-3.617778e-02, -7.235830e-02) => 3.333501e-01\n",
      "b_C (-2.553058e-02, -5.106147e-02) => 3.333360e-01\n",
      "b_C (-1.000121e-01, -2.000541e-01) => 3.333996e-01\n",
      "b_C (-3.617778e-02, -7.235830e-02) => 3.333501e-01\n",
      "b_C (2.769898e-01, 5.539746e-01) => 3.333293e-01\n",
      "b_C (-9.507479e-02, -1.901528e-01) => 3.333409e-01\n",
      "b_C (-6.398345e-01, -1.000000e+00) => 2.196353e-01\n",
      "b_o (3.986374e-03, 7.972513e-03) => 3.333203e-01\n",
      "b_o (1.554313e-02, 3.108626e-02) => 3.333332e-01\n",
      "b_o (2.506209e-02, 5.012415e-02) => 3.333330e-01\n",
      "b_o (-4.606326e-02, -9.212753e-02) => 3.333381e-01\n",
      "b_o (-1.861366e-02, -3.722747e-02) => 3.333352e-01\n",
      "b_o (1.010451e-02, 2.020890e-02) => 3.333306e-01\n",
      "b_o (-1.620602e-01, -3.241204e-01) => 3.333333e-01\n",
      "b_o (1.010451e-02, 2.020890e-02) => 3.333306e-01\n",
      "b_o (-9.748683e-02, -1.949740e-01) => 3.333342e-01\n",
      "b_o (6.428747e-03, 1.285736e-02) => 3.333288e-01\n",
      "b_y (3.350139e-03, 6.700243e-03) => 3.333310e-01\n",
      "b_y (1.699539e-02, 3.399061e-02) => 3.333311e-01\n",
      "b_y (1.051753e-02, 2.103495e-02) => 3.333311e-01\n",
      "b_y (1.259531e-03, 2.519049e-03) => 3.333310e-01\n",
      "b_y (5.657531e-03, 1.131501e-02) => 3.333311e-01\n",
      "b_y (1.118971e-02, 2.237931e-02) => 3.333311e-01\n",
      "b_y (2.943673e-02, 5.887318e-02) => 3.333312e-01\n",
      "b_y (1.068071e-01, 2.136131e-01) => 3.333312e-01\n",
      "b_y (1.039168e-02, 2.078325e-02) => 3.333311e-01\n",
      "b_y (-1.283901e-01, -2.567825e-01) => 3.333374e-01\n"
     ]
    }
   ],
   "source": [
    "gradient_check(inputs, targets, g_h_prev, g_C_prev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
