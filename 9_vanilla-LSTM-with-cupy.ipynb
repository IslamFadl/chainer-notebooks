{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla LSTM with CuPy\n",
    "\n",
    "This is a port of [Vanilla LSTM with numpy](http://blog.varunajayasiri.com/numpy_lstm.html) that shows how to run the numpy-based machine learning code on GPU using CuPy.\n",
    "\n",
    "This "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100 99993  100 99993    0     0   130k      0 --:--:-- --:--:-- --:--:--  157k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -L -O http://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('shakespear.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data and calculate indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 99993 characters, 62 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions and Derivatives\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\sigma(x) &=& \\frac{1}{1 + e^{-x}} \\\\\n",
    "\\frac{d \\sigma(x)}{d x} &=& \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "### Tanh\n",
    "\n",
    "$$\n",
    "\\frac{d \\tanh(x)}{dx} = 1 - \\tanh^2(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + cp.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return cp.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights\n",
    "\n",
    "We use random weights with normal distribution `(0, weight_sd)` for `tanh` activation function and `(0.5, weight_sd)` for `sigmoid` activation function.\n",
    "\n",
    "Biases are initialized to zeros.\n",
    "\n",
    "Formulae for LSTM are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_f = cp.random.randn(H_size, z_size) * weight_sd + 0.5\n",
    "b_f = cp.zeros((H_size, 1))\n",
    "\n",
    "W_i = cp.random.randn(H_size, z_size) * weight_sd + 0.5\n",
    "b_i = cp.zeros((H_size, 1))\n",
    "\n",
    "W_C = cp.random.randn(H_size, z_size) * weight_sd\n",
    "b_C = cp.zeros((H_size, 1))\n",
    "\n",
    "W_o = cp.random.randn(H_size, z_size) * weight_sd + 0.5\n",
    "b_o = cp.zeros((H_size, 1))\n",
    "\n",
    "#For final layer to predict the next character\n",
    "W_y = cp.random.randn(X_size, H_size) * weight_sd\n",
    "b_y = cp.zeros((X_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dW_f = cp.zeros_like(W_f)\n",
    "dW_i = cp.zeros_like(W_i)\n",
    "dW_C = cp.zeros_like(W_C)\n",
    "\n",
    "dW_o = cp.zeros_like(W_o)\n",
    "dW_y = cp.zeros_like(W_y)\n",
    "\n",
    "db_f = cp.zeros_like(b_f)\n",
    "db_i = cp.zeros_like(b_i)\n",
    "db_C = cp.zeros_like(b_C)\n",
    "\n",
    "db_o = cp.zeros_like(b_o)\n",
    "db_y = cp.zeros_like(b_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "\n",
    "Image taken from [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). Please read the article for a good explanation of LSTMs.\n",
    "\n",
    "### Concatenation of $h_{t-1}$ and $x_t$\n",
    "\n",
    "$$\n",
    "z = [h_{t-1}, x_t]\n",
    "$$\n",
    "\n",
    "### LSTM functions\n",
    "\n",
    "$$\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)\n",
    "    assert C_prev.shape == (H_size, 1)\n",
    "\n",
    "    z = cp.row_stack((h_prev, x))\n",
    "    f = sigmoid(cp.dot(W_f, z) + b_f)\n",
    "    i = sigmoid(cp.dot(W_i, z) + b_i)\n",
    "    C_bar = tanh(cp.dot(W_C, z) + b_C)\n",
    "\n",
    "    C = f * C_prev + i * C_bar\n",
    "    o = sigmoid(cp.dot(W_o, z) + b_o)\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    y = cp.dot(W_y, h) + b_y\n",
    "    p = cp.exp(y) / cp.sum(cp.exp(y))\n",
    "\n",
    "    return z, f, i, C_bar, C, o, h, y, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev, z, f, i, C_bar, C, o, h, y, p):\n",
    "\n",
    "    global dW_f, dW_i, dW_C, dW_o, dW_y\n",
    "    global db_f, db_i, db_C, db_o, db_y\n",
    "\n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    assert p.shape == (X_size, 1)\n",
    "\n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "\n",
    "    dy = cp.copy(p)\n",
    "    dy[target] -= 1\n",
    "\n",
    "    dW_y += cp.dot(dy, h.T)\n",
    "    db_y += dy\n",
    "\n",
    "    dh = cp.dot(W_y.T, dy)\n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    dW_o += cp.dot(do, z.T)\n",
    "    db_o += do\n",
    "\n",
    "    dC = cp.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dC_bar * dtanh(C_bar)\n",
    "    dW_C += cp.dot(dC_bar, z.T)\n",
    "    db_C += dC_bar\n",
    "\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    dW_i += cp.dot(di, z.T)\n",
    "    db_i += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    dW_f += cp.dot(df, z.T)\n",
    "    db_f += df\n",
    "\n",
    "    dz = cp.dot(W_f.T, df) \\\n",
    "        + cp.dot(W_i.T, di) \\\n",
    "        + cp.dot(W_C.T, dC_bar) \\\n",
    "        + cp.dot(W_o.T, do)\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "\n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, f_s, i_s, C_bar_s, C_s, o_s, h_s, y_s, p_s = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    # Values at t - 1\n",
    "    h_s[-1] = cp.copy(h_prev)\n",
    "    C_s[-1] = cp.copy(C_prev)\n",
    "\n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = cp.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "\n",
    "        z_s[t], f_s[t], i_s[t], C_bar_s[t], C_s[t], o_s[t], h_s[t], y_s[t], p_s[t] \\\n",
    "            = forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "\n",
    "        loss += -cp.log(p_s[t][targets[t], 0]) # Loss for at t\n",
    "\n",
    "\n",
    "    for dparam in [dW_f, dW_i, dW_C, dW_o, dW_y, db_f, db_i, db_C, db_o, db_y]:\n",
    "        dparam.fill(0)\n",
    "\n",
    "    dh_next = cp.zeros_like(h_s[0]) #dh from the next character\n",
    "    dC_next = cp.zeros_like(C_s[0]) #dh from the next character\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        dh_next, dC_next = backward(target = targets[t], dh_next = dh_next, dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                 z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t], C = C_s[t], o = o_s[t],\n",
    "                 h = h_s[t], y = y_s[t], p = p_s[t])\n",
    "\n",
    "    # Clip gradients to mitigate exploding gradients\n",
    "    for dparam in [dW_f, dW_i, dW_C, dW_o, dW_y, db_f, db_i, db_C, db_o, db_y]:\n",
    "        cp.clip(dparam, -1, 1, out=dparam)\n",
    "\n",
    "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
    "    x = cp.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    h = h_prev\n",
    "    C = C_prev\n",
    "\n",
    "    indexes = []\n",
    "\n",
    "    for t in range(sentence_length):\n",
    "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
    "        idx = cp.random.choice(range(X_size), p=p.ravel())\n",
    "        x = cp.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, C_prev):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "\n",
    "    # Get predictions for 200 letters with current model\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mW_f = cp.zeros_like(W_f)\n",
    "mW_i = cp.zeros_like(W_i)\n",
    "mW_C = cp.zeros_like(W_C)\n",
    "mW_o = cp.zeros_like(W_o)\n",
    "mW_y = cp.zeros_like(W_y)\n",
    "\n",
    "mb_f = cp.zeros_like(b_f)\n",
    "mb_i = cp.zeros_like(b_i)\n",
    "mb_C = cp.zeros_like(b_C)\n",
    "mb_o = cp.zeros_like(b_o)\n",
    "mb_y = cp.zeros_like(b_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -cp.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, p = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = cp.zeros((0))\n",
    "plot_loss = cp.zeros((0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cupy' has no attribute 'row_stack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-74f54995932d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mT_steps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_h_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_C_prev\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_h_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_C_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-4a6491b7cc4a>\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(inputs, targets, h_prev, C_prev)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# Input character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mz_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_bar_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m             \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Loss for at t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ac959cceee74>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(x, h_prev, C_prev)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mC_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cupy' has no attribute 'row_stack'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Try catch for interruption\n",
    "    try:\n",
    "        # Reset\n",
    "        if p + T_steps >= len(data) or iteration == 0:\n",
    "            g_h_prev = cp.zeros((H_size, 1))\n",
    "            g_C_prev = cp.zeros((H_size, 1))\n",
    "            p = 0\n",
    "\n",
    "\n",
    "        inputs = [char_to_idx[ch] for ch in data[p: p + T_steps]]\n",
    "        targets = [char_to_idx[ch] for ch in data[p + 1: p + T_steps + 1]]\n",
    "\n",
    "        loss, g_h_prev, g_C_prev =  forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "        # Print every hundred steps\n",
    "        if iteration % 100 == 0:\n",
    "            update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "\n",
    "        # Update weights\n",
    "        for param, dparam, mem in zip([W_f, W_i, W_C, W_o, W_y, b_f, b_i, b_C, b_o, b_y],\n",
    "                                      [dW_f, dW_i, dW_C, dW_o, dW_y, db_f, db_i, db_C, db_o, db_y],\n",
    "                                      [mW_f, mW_i, mW_C, mW_o, mW_y, mb_f, mb_i, mb_C, mb_o, mb_y]):\n",
    "            mem += dparam * dparam # Calculate sum of gradients\n",
    "            #print(learning_rate * dparam)\n",
    "            param += -(learning_rate * dparam / cp.sqrt(mem + 1e-8))\n",
    "\n",
    "        plot_iter = cp.append(plot_iter, [iteration])\n",
    "        plot_loss = cp.append(plot_loss, [loss])\n",
    "\n",
    "        p += T_steps\n",
    "        iteration += 1\n",
    "    except KeyboardInterrupt:\n",
    "        update_status(inputs, g_h_prev, g_C_prev)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
